{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Aspect-Based Sentiment Analysis model\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "absa_model = AutoModelForSequenceClassification \\\n",
    "  .from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\").cuda()\n",
    "\n",
    "# Load a traditional Sentiment Analysis model\n",
    "sentiment_model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=sentiment_model_path,\n",
    "                          tokenizer=sentiment_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,   279,  4340,   284,   431,  1800,   901,   262,   710,\n",
       "           265,   291,  1905,   412,   278, 62894,     2,  1800,     2,     2]],\n",
       "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_MutableMapping__marker',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__ior__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_encodings',\n",
       " '_n_sequences',\n",
       " 'char_to_token',\n",
       " 'char_to_word',\n",
       " 'clear',\n",
       " 'convert_to_tensors',\n",
       " 'copy',\n",
       " 'data',\n",
       " 'encodings',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'is_fast',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'n_sequences',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'sequence_ids',\n",
       " 'setdefault',\n",
       " 'to',\n",
       " 'token_to_chars',\n",
       " 'token_to_sequence',\n",
       " 'token_to_word',\n",
       " 'tokens',\n",
       " 'update',\n",
       " 'values',\n",
       " 'word_ids',\n",
       " 'word_to_chars',\n",
       " 'word_to_tokens',\n",
       " 'words']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.1935, -2.0749,  3.7412]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The shirt was really comfortable however the price of this item made it untouchable\n",
      "\n",
      "Sentiment of aspect 'comfortable' is:\n",
      "Label negative: 0.3936208188533783\n",
      "Label neutral: 0.08943913131952286\n",
      "Label positive: 0.5169400572776794\n",
      "\n",
      "Sentiment of aspect 'shirt' is:\n",
      "Label negative: 0.0026312118861824274\n",
      "Label neutral: 0.002962348982691765\n",
      "Label positive: 0.9944064617156982\n",
      "\n",
      "Overall sentiment: negative with score 0.5427278280258179\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The shirt was really comfortable however the price of this item made it untouchable\"\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print()\n",
    "\n",
    "# ABSA of \"food\"\n",
    "aspect = \"comfortable\"\n",
    "inputs = absa_tokenizer(f\"[CLS] {sentence} [SEP] {aspect} [SEP]\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = absa_model(**inputs)\n",
    "probs = F.softmax(outputs.logits, dim=1)\n",
    "probs = probs.cpu().detach().numpy()[0]\n",
    "print(f\"Sentiment of aspect '{aspect}' is:\")\n",
    "for prob, label in zip(probs, [\"negative\", \"neutral\", \"positive\"]):\n",
    "  print(f\"Label {label}: {prob}\")\n",
    "print()\n",
    "# Sentiment of aspect 'food' is:\n",
    "# Label negative: 0.0009989114478230476s\n",
    "# Label neutral: 0.001823813421651721\n",
    "# Label positive: 0.997177243232727\n",
    "\n",
    "# ABSA of \"service\"\n",
    "aspect = \"shirt\"\n",
    "inputs = absa_tokenizer(f\"[CLS] {sentence} [SEP] {aspect} [SEP]\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = absa_model(**inputs)\n",
    "probs = F.softmax(outputs.logits, dim=1)\n",
    "probs = probs.detach().numpy()[0]\n",
    "print(f\"Sentiment of aspect '{aspect}' is:\")\n",
    "for prob, label in zip(probs, [\"negative\", \"neutral\", \"positive\"]):\n",
    "  print(f\"Label {label}: {prob}\")\n",
    "print()\n",
    "# Sentiment of aspect 'service' is:\n",
    "# Label negative: 0.9946129322052002\n",
    "# Label neutral: 0.002369985682889819\n",
    "# Label positive: 0.003017079783603549\n",
    "\n",
    "# Overall sentiment of the sentence\n",
    "sentiment = sentiment_model([sentence])[0]\n",
    "print(f\"Overall sentiment: {sentiment['label']} with score {sentiment['score']}\")\n",
    "# Overall sentiment: Negative with score 0.7706006765365601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95795</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>01 29, 2014</td>\n",
       "      <td>A2LUD665NACE4F</td>\n",
       "      <td>B001LRJX4U</td>\n",
       "      <td>Lovetoshop</td>\n",
       "      <td>I bought these for my husband who is a metal c...</td>\n",
       "      <td>Excellent quality</td>\n",
       "      <td>1390953600</td>\n",
       "      <td>{'Size:': ' Medium', 'Color:': ' Dark Navy'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141332</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>09 15, 2015</td>\n",
       "      <td>AS1MJFB1CKAKE</td>\n",
       "      <td>B005934WR0</td>\n",
       "      <td>Brit</td>\n",
       "      <td>Our daughter has wore these boots until she ha...</td>\n",
       "      <td>Worn Rain, Snow, and Shine</td>\n",
       "      <td>1442275200</td>\n",
       "      <td>{'Size:': ' 7 M US Toddler', 'Color:': ' Wings'}</td>\n",
       "      <td>['https://images-na.ssl-images-amazon.com/imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34909</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>01 9, 2016</td>\n",
       "      <td>A3JEXQ0P7K8P5T</td>\n",
       "      <td>B000XBM1L2</td>\n",
       "      <td>Jennifer P.</td>\n",
       "      <td>Bought them for my boyfriend and he didn't car...</td>\n",
       "      <td>I love the way they look but ended up giving t...</td>\n",
       "      <td>1452297600</td>\n",
       "      <td>{'Size:': ' 10.5 D(M) US', 'Color:': ' Black'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319066</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>09 7, 2016</td>\n",
       "      <td>A2FR7O6RY9PLY1</td>\n",
       "      <td>B01G87QKMA</td>\n",
       "      <td>Pig Scum</td>\n",
       "      <td>Made my hubbies eyeballs poke out on their ste...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1473206400</td>\n",
       "      <td>{'Size:': ' XX-Large', 'Color:': ' Floral'}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30919</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>10 13, 2017</td>\n",
       "      <td>A27BHHRBAPQ9NZ</td>\n",
       "      <td>B000XBM1L2</td>\n",
       "      <td>Maria Roberts</td>\n",
       "      <td>Very comfortable and durable. Material does st...</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>1507852800</td>\n",
       "      <td>{'Size:': ' 10 B(M) US', 'Color:': ' Black Mor...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall  vote  verified   reviewTime      reviewerID        asin   \n",
       "95795       5.0   NaN      True  01 29, 2014  A2LUD665NACE4F  B001LRJX4U  \\\n",
       "141332      5.0   5.0      True  09 15, 2015   AS1MJFB1CKAKE  B005934WR0   \n",
       "34909       5.0   NaN      True   01 9, 2016  A3JEXQ0P7K8P5T  B000XBM1L2   \n",
       "319066      5.0   NaN      True   09 7, 2016  A2FR7O6RY9PLY1  B01G87QKMA   \n",
       "30919       4.0   NaN      True  10 13, 2017  A27BHHRBAPQ9NZ  B000XBM1L2   \n",
       "\n",
       "         reviewerName                                         reviewText   \n",
       "95795      Lovetoshop  I bought these for my husband who is a metal c...  \\\n",
       "141332           Brit  Our daughter has wore these boots until she ha...   \n",
       "34909     Jennifer P.  Bought them for my boyfriend and he didn't car...   \n",
       "319066       Pig Scum  Made my hubbies eyeballs poke out on their ste...   \n",
       "30919   Maria Roberts  Very comfortable and durable. Material does st...   \n",
       "\n",
       "                                                  summary  unixReviewTime   \n",
       "95795                                   Excellent quality      1390953600  \\\n",
       "141332                         Worn Rain, Snow, and Shine      1442275200   \n",
       "34909   I love the way they look but ended up giving t...      1452297600   \n",
       "319066                                         Five Stars      1473206400   \n",
       "30919                                          Four Stars      1507852800   \n",
       "\n",
       "                                                    style   \n",
       "95795        {'Size:': ' Medium', 'Color:': ' Dark Navy'}  \\\n",
       "141332   {'Size:': ' 7 M US Toddler', 'Color:': ' Wings'}   \n",
       "34909      {'Size:': ' 10.5 D(M) US', 'Color:': ' Black'}   \n",
       "319066        {'Size:': ' XX-Large', 'Color:': ' Floral'}   \n",
       "30919   {'Size:': ' 10 B(M) US', 'Color:': ' Black Mor...   \n",
       "\n",
       "                                                    image  \n",
       "95795                                                 NaN  \n",
       "141332  ['https://images-na.ssl-images-amazon.com/imag...  \n",
       "34909                                                 NaN  \n",
       "319066                                                NaN  \n",
       "30919                                                 NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = pd.read_csv('../Project_Data/Review_data.csv', index_col=0)\n",
    "review_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review :  Good seller, good product.\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.004383445251733065\n",
      "Label neutral: 0.018631240352988243\n",
      "Label positive: 0.9769853353500366\n",
      "\n",
      "Overall sentiment: positive with score 0.8653029203414917\n",
      "Review :  made well.\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.0025922670029103756\n",
      "Label neutral: 0.0130251944065094\n",
      "Label positive: 0.9843824505805969\n",
      "\n",
      "Overall sentiment: positive with score 0.7556275725364685\n",
      "Review :  They should say if your head is bigger than 7\" this product is a tight fit! Great product works well but a bit tight!\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.9587574601173401\n",
      "Label neutral: 0.013137195259332657\n",
      "Label positive: 0.028105447068810463\n",
      "\n",
      "Overall sentiment: positive with score 0.7290250062942505\n",
      "Review :  half size might have been good\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.4579278528690338\n",
      "Label neutral: 0.3043316602706909\n",
      "Label positive: 0.23774059116840363\n",
      "\n",
      "Overall sentiment: neutral with score 0.3694361746311188\n",
      "Review :  will buy them again\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.003401422407478094\n",
      "Label neutral: 0.01899215206503868\n",
      "Label positive: 0.9776064157485962\n",
      "\n",
      "Overall sentiment: positive with score 0.5294457674026489\n",
      "Review :  This dress is now the prettiest dress I own, except it doesn't fit very well for plus size women.  Need to lose some weight to be able to wear.  The dress gives me incentive.  The details and embroidery are gorgeous!\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.004730964079499245\n",
      "Label neutral: 0.010279019363224506\n",
      "Label positive: 0.9849900007247925\n",
      "\n",
      "Overall sentiment: negative with score 0.4614996910095215\n",
      "Review :  My akb loves these. Wears them everyday to school.\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.003340145107358694\n",
      "Label neutral: 0.03210236132144928\n",
      "Label positive: 0.9645574688911438\n",
      "\n",
      "Overall sentiment: positive with score 0.8495296239852905\n",
      "Review :  Good bra, no cup overflow, comfortable. Seems to be holding up well, have had for several months.\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.007441391237080097\n",
      "Label neutral: 0.07814519852399826\n",
      "Label positive: 0.9144133925437927\n",
      "\n",
      "Overall sentiment: positive with score 0.8793109655380249\n",
      "Review :  How can you not like it?\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.004288188647478819\n",
      "Label neutral: 0.03769558295607567\n",
      "Label positive: 0.9580162763595581\n",
      "\n",
      "Overall sentiment: negative with score 0.5617985129356384\n",
      "Review :  Poor quality and there are already scuff marks. Very unhappy.\n",
      "Sentiment of aspect 'price' is:\n",
      "Label negative: 0.9341709017753601\n",
      "Label neutral: 0.04653776064515114\n",
      "Label positive: 0.019291305914521217\n",
      "\n",
      "Overall sentiment: negative with score 0.9608160257339478\n"
     ]
    }
   ],
   "source": [
    "aspect = \"price\"\n",
    "\n",
    "reviews = review_data['reviewText'].sample(10).tolist()\n",
    "for review in reviews:\n",
    "  inputs = absa_tokenizer(f\"[CLS] {review} [SEP] {aspect} [SEP]\", return_tensors=\"pt\").to('cuda')\n",
    "  outputs = absa_model(**inputs)\n",
    "  probs = F.softmax(outputs.logits, dim=1)\n",
    "  probs = probs.cpu().detach().numpy()[0]\n",
    "  print(\"Review : \", review)\n",
    "  print(f\"Sentiment of aspect '{aspect}' is:\")\n",
    "  for prob, label in zip(probs, [\"negative\", \"neutral\", \"positive\"]):\n",
    "    print(f\"Label {label}: {prob}\")\n",
    "  print()\n",
    "  sentiment = sentiment_model([review])[0]\n",
    "  print(f\"Overall sentiment: {sentiment['label']} with score {sentiment['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 1301/98988 [00:56<1:11:02, 22.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m   inputs \u001b[39m=\u001b[39m absa_tokenizer(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[CLS] \u001b[39m\u001b[39m{\u001b[39;00msentence\u001b[39m}\u001b[39;00m\u001b[39m [SEP] \u001b[39m\u001b[39m{\u001b[39;00maspect\u001b[39m}\u001b[39;00m\u001b[39m [SEP]\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m   \u001b[39m# input_list.append(inputs)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m   outputs \u001b[39m=\u001b[39m absa_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m     10\u001b[0m \u001b[39m# outputs = absa_model(torch.Tensor(input_list))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m   \n\u001b[0;32m     12\u001b[0m \u001b[39m# probs = F.softmax(outputs.logits, dim=1)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# probs = probs.cpu().detach().numpy()[0]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# prob_list.append(probs)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m   \u001b[39m# return prob_list\u001b[39;00m\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1311\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1303\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1311\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[0;32m   1312\u001b[0m     input_ids,\n\u001b[0;32m   1313\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1314\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1315\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1316\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1317\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1318\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1319\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1320\u001b[0m )\n\u001b[0;32m   1322\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1323\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m   1075\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1076\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1077\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1080\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1081\u001b[0m )\n\u001b[1;32m-> 1083\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1084\u001b[0m     embedding_output,\n\u001b[0;32m   1085\u001b[0m     attention_mask,\n\u001b[0;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1090\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:521\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    512\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    513\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    514\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m         rel_embeddings,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    522\u001b[0m         next_kv,\n\u001b[0;32m    523\u001b[0m         attention_mask,\n\u001b[0;32m    524\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    525\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    526\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    527\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    530\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    531\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    354\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    355\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m ):\n\u001b[1;32m--> 362\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    363\u001b[0m         hidden_states,\n\u001b[0;32m    364\u001b[0m         attention_mask,\n\u001b[0;32m    365\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    366\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    367\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    368\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    371\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    286\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m ):\n\u001b[1;32m--> 293\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    294\u001b[0m         hidden_states,\n\u001b[0;32m    295\u001b[0m         attention_mask,\n\u001b[0;32m    296\u001b[0m         output_attentions,\n\u001b[0;32m    297\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    298\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    299\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    302\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:715\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    713\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_proj(query_states), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads)\n\u001b[0;32m    714\u001b[0m key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_proj(hidden_states), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads)\n\u001b[1;32m--> 715\u001b[0m value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_proj(hidden_states), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads)\n\u001b[0;32m    717\u001b[0m rel_att \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def sentiment_analysis(reviews, aspect):\n",
    "reviews = mini_review_df['reviewText'].to_list()\n",
    "aspect = 'cost'\n",
    "# input_list = []\n",
    "# prob_list = []\n",
    "for sentence in tqdm(reviews):\n",
    "  inputs = absa_tokenizer(f\"[CLS] {sentence} [SEP] {aspect} [SEP]\", return_tensors=\"pt\").to(\"cuda\")\n",
    "  # input_list.append(inputs)\n",
    "  outputs = absa_model(**inputs)\n",
    "# outputs = absa_model(torch.Tensor(input_list))\n",
    "  \n",
    "# probs = F.softmax(outputs.logits, dim=1)\n",
    "# probs = probs.cpu().detach().numpy()[0]\n",
    "# prob_list.append(probs)\n",
    "  # return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Invalid device id",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_name(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\cuda\\__init__.py:365\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     \u001b[39mreturn\u001b[39;00m get_device_properties(device)\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\cuda\\__init__.py:398\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    396\u001b[0m device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    397\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n\u001b[1;32m--> 398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid device id\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    399\u001b[0m \u001b[39mreturn\u001b[39;00m _get_device_properties(device)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Invalid device id"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,   273,   590,   280,   297,   409,   264,   794,   360,\n",
       "           419,  1794,   270,   266,  1948, 11401,   261,   304,   273,   431,\n",
       "           849,   266,   397,   714,   263,   262,  1937,  5172,   272,   260,\n",
       "           471,   273, 60999,   321,   262,  6737,   270,   311,   263,   306,\n",
       "           332,   423,   300,   325,  4662, 25880,   324,   343,   280,   268,\n",
       "           363,  8783, 20547,   263,   278,   327,  9253, 18529,   271, 70032,\n",
       "           260,   273,  3233,   266,   467,  1948, 11401,   292, 10328,   263,\n",
       "           291,   269,   293,   659,   312,  1237,   265,   262,   375,   260,\n",
       "           279, 10328,   311,  4511,   312, 67672,   268,   267,   324,   400,\n",
       "           278,   412,   272,  8496, 67672, 11953,   764,   262, 11401,   683,\n",
       "           272,   280,   268,  3979,   267,   731,  1941,   271, 12255,   341,\n",
       "         75548,   341,   263,   306,   280,   368,  2902,   264,   282,   262,\n",
       "           454, 11401,   884,   324,   424,  1805,   260,  1366, 35270,   294,\n",
       "           291,   311,   327, 41110,   268, 11726,   287,  6446,   564,   292,\n",
       "           262,  1741,   263,   374,   290,   384,   322,   261,   278,   296,\n",
       "          1340,   300,   285,   263,   269,  1850,   261,  1850,  1800,   260,\n",
       "           273,   333,  1437,   262,   710,   332,   625,   324,   273,   387,\n",
       "           809,   266,   567,   261,   304,   273,   280,   436,   286,   264,\n",
       "           552,   272,   267,   312,  1753,   260,  7448,   261,   283,   266,\n",
       "         43538,  8334,   275,   266,   614,  4458,   261,   278,   284,  1231,\n",
       "           262,  1578,   267,  2352,   263,   971,   260,     2,   751,     2,\n",
       "             2]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344920"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5543/5543 [01:43<00:00, 53.42it/s]\n"
     ]
    }
   ],
   "source": [
    "asins = review_data['asin'].unique()\n",
    "per_asin_reviews = 30\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for asin in tqdm(asins):\n",
    "    mini_df = review_data[review_data['asin'] == asin]\n",
    "    if mini_df.shape[0] < per_asin_reviews:\n",
    "        df_list.append(mini_df.copy())\n",
    "    else:\n",
    "        df_list.append(mini_df.sample(per_asin_reviews).copy())\n",
    "        \n",
    "mini_review_df = pd.concat(df_list)\n",
    "mini_review_df.to_csv('../Project_Data/MiniReview.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_review_df.to_csv('../Project_Data/MiniReview.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344920, 12)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1651/344920 [01:09<4:00:15, 23.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 2\u001b[0m results \u001b[39m=\u001b[39m sentiment_analysis(review_data[\u001b[39m'\u001b[39;49m\u001b[39mreviewText\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto_list(), aspect\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcost\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[59], line 5\u001b[0m, in \u001b[0;36msentiment_analysis\u001b[1;34m(reviews, aspect)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tqdm(reviews):\n\u001b[0;32m      4\u001b[0m   inputs \u001b[39m=\u001b[39m absa_tokenizer(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[CLS] \u001b[39m\u001b[39m{\u001b[39;00msentence\u001b[39m}\u001b[39;00m\u001b[39m [SEP] \u001b[39m\u001b[39m{\u001b[39;00maspect\u001b[39m}\u001b[39;00m\u001b[39m [SEP]\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m   outputs \u001b[39m=\u001b[39m absa_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m      6\u001b[0m   probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m   probs \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1311\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1303\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1305\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1311\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[0;32m   1312\u001b[0m     input_ids,\n\u001b[0;32m   1313\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1314\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1315\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1316\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1317\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1318\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1319\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1320\u001b[0m )\n\u001b[0;32m   1322\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1323\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m   1075\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1076\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1077\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1080\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1081\u001b[0m )\n\u001b[1;32m-> 1083\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1084\u001b[0m     embedding_output,\n\u001b[0;32m   1085\u001b[0m     attention_mask,\n\u001b[0;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1087\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1090\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1092\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:521\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    512\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    513\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    514\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m         rel_embeddings,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    522\u001b[0m         next_kv,\n\u001b[0;32m    523\u001b[0m         attention_mask,\n\u001b[0;32m    524\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    525\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    526\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    527\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    530\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    531\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    354\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    355\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m ):\n\u001b[1;32m--> 362\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    363\u001b[0m         hidden_states,\n\u001b[0;32m    364\u001b[0m         attention_mask,\n\u001b[0;32m    365\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    366\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    367\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    368\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    371\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    285\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    286\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m ):\n\u001b[1;32m--> 293\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    294\u001b[0m         hidden_states,\n\u001b[0;32m    295\u001b[0m         attention_mask,\n\u001b[0;32m    296\u001b[0m         output_attentions,\n\u001b[0;32m    297\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[0;32m    298\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[0;32m    299\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    302\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:728\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[0;32m    727\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[1;32m--> 728\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[0;32m    729\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[0;32m    730\u001b[0m     )\n\u001b[0;32m    732\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    733\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[1;32md:\\Softwares\\Installed\\Anaconda\\envs\\isr_project\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:815\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[1;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39m# position->content\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mp2c\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_att_type:\n\u001b[1;32m--> 815\u001b[0m     scale \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(torch\u001b[39m.\u001b[39mtensor(pos_query_layer\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat) \u001b[39m*\u001b[39m scale_factor)\n\u001b[0;32m    816\u001b[0m     \u001b[39mif\u001b[39;00m key_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m!=\u001b[39m query_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m    817\u001b[0m         r_pos \u001b[39m=\u001b[39m build_relative_position(\n\u001b[0;32m    818\u001b[0m             key_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[0;32m    819\u001b[0m             key_layer\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m             device\u001b[39m=\u001b[39mquery_layer\u001b[39m.\u001b[39mdevice,\n\u001b[0;32m    823\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "results = sentiment_analysis(review_data['reviewText'].to_list(), aspect='cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [eval(cat) for cat in metadata['category'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_set = set()\n",
    "list_of_lists = categories\n",
    "joined_catlist = [item for sublist in list_of_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_catlist = [item for item in joined_catlist if len(item.split(' '))<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " [Text(0, 0, 'Clothing, Shoes & Jewelry'),\n",
       "  Text(1, 0, 'Women'),\n",
       "  Text(2, 0, 'Clothing'),\n",
       "  Text(3, 0, 'Men'),\n",
       "  Text(4, 0, 'Shoes'),\n",
       "  Text(5, 0, 'Accessories'),\n",
       "  Text(6, 0, 'Jewelry'),\n",
       "  Text(7, 0, 'Novelty & More'),\n",
       "  Text(8, 0, 'Imported'),\n",
       "  Text(9, 0, 'Novelty'),\n",
       "  Text(10, 0, 'Girls'),\n",
       "  Text(11, 0, 'Shirts'),\n",
       "  Text(12, 0, 'T-Shirts'),\n",
       "  Text(13, 0, 'Athletic'),\n",
       "  Text(14, 0, 'Sandals'),\n",
       "  Text(15, 0, 'Boots'),\n",
       "  Text(16, 0, 'Lingerie, Sleep & Lounge'),\n",
       "  Text(17, 0, 'Costumes & Accessories'),\n",
       "  Text(18, 0, 'Boys'),\n",
       "  Text(19, 0, 'Watches')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAJBCAYAAABYj48LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACEwElEQVR4nO3dd1RU19oG8GfovYgKolIUGypiiRFNLIgiGks0JlHsqLFhIRo1xRZrEhVbYmIDNfYYjV2jAqJYgoK9oChYwBZAEFFgf3/4cS4jaGTmjHD0+a01617OHN/ZQ5iZZ/bZRSWEECAiIiJSEL3ibgARERFRUTHAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIYFHcDdCU3Nxe3b9+GpaUlVCpVcTeHiIiIXoMQAo8ePYKjoyP09F7ez/LWBpjbt2+jYsWKxd0MIiIi0kBiYiIqVKjw0vvf2gBjaWkJ4PkvwMrKqphbQ0RERK8jLS0NFStWlD7HX+atDTB5l42srKwYYIiIiBTmv4Z/cBAvERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESmOQXE3QIlcxu2Qrdb1me1kq0VERPSuYA8MERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKU6RAsykSZOgUqnUbtWrV5fuf/LkCYYOHQo7OztYWFigS5cuSE5OVquRkJCAdu3awczMDGXLlsWYMWOQnZ2tdk5YWBjq1asHY2NjuLm5ISQkRPNnSERERG+dIvfA1KxZE3fu3JFukZGR0n2jRo3Ctm3bsHHjRoSHh+P27dvo3LmzdH9OTg7atWuHp0+f4siRIwgNDUVISAgmTJggnRMfH4927dqhRYsWiImJwciRI9G/f3/s2bNHy6dKREREbwuDIv8DAwM4ODgUOJ6amoply5ZhzZo18Pb2BgCsWLECNWrUwNGjR9GoUSPs3bsX58+fx99//w17e3t4enri+++/x9ixYzFp0iQYGRlh8eLFcHV1xezZswEANWrUQGRkJObOnQtfX18tny4RERG9DYrcA3PlyhU4OjqiUqVK8Pf3R0JCAgAgOjoaz549g4+Pj3Ru9erV4eTkhKioKABAVFQUateuDXt7e+kcX19fpKWl4dy5c9I5+WvknZNXg4iIiKhIPTDvv/8+QkJCUK1aNdy5cweTJ0/Ghx9+iLNnzyIpKQlGRkawsbFR+zf29vZISkoCACQlJamFl7z78+571TlpaWnIzMyEqalpoW3LyspCVlaW9HNaWlpRnhoREREpSJECjJ+fn/T/PTw88P7778PZ2RkbNmx4abB4U2bMmIHJkycXaxuIiIjozdBqGrWNjQ2qVq2KuLg4ODg44OnTp0hJSVE7Jzk5WRoz4+DgUGBWUt7P/3WOlZXVK0PS+PHjkZqaKt0SExO1eWpERERUgmkVYNLT03H16lWUK1cO9evXh6GhIfbv3y/df+nSJSQkJMDLywsA4OXlhTNnzuDu3bvSOfv27YOVlRXc3d2lc/LXyDsnr8bLGBsbw8rKSu1GREREb6ciBZjRo0cjPDwc169fx5EjR/Dxxx9DX18f3bp1g7W1NQICAhAUFISDBw8iOjoaffv2hZeXFxo1agQAaN26Ndzd3dGzZ0/ExsZiz549+PbbbzF06FAYGxsDAAYNGoRr167hq6++wsWLF/Hzzz9jw4YNGDVqlPzPnoiIiBSpSGNgbt68iW7duuHBgwcoU6YMPvjgAxw9ehRlypQBAMydOxd6enro0qULsrKy4Ovri59//ln69/r6+ti+fTsGDx4MLy8vmJubo3fv3pgyZYp0jqurK3bs2IFRo0Zh3rx5qFChApYuXcop1ERERCRRCSFEcTdCF9LS0mBtbY3U1FTZLye5jNshW63rM9vJVouIiEjpXvfzm3shERERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiMMAQERGR4jDAEBERkeIwwBAREZHiaBVgZs6cCZVKhZEjR0rHnjx5gqFDh8LOzg4WFhbo0qULkpOT1f5dQkIC2rVrBzMzM5QtWxZjxoxBdna22jlhYWGoV68ejI2N4ebmhpCQEG2aSkRERG8RjQPMiRMn8Ouvv8LDw0Pt+KhRo7Bt2zZs3LgR4eHhuH37Njp37izdn5OTg3bt2uHp06c4cuQIQkNDERISggkTJkjnxMfHo127dmjRogViYmIwcuRI9O/fH3v27NG0uURERPQW0SjApKenw9/fH0uWLIGtra10PDU1FcuWLcOcOXPg7e2N+vXrY8WKFThy5AiOHj0KANi7dy/Onz+P1atXw9PTE35+fvj++++xaNEiPH36FACwePFiuLq6Yvbs2ahRowaGDRuGTz75BHPnzpXhKRMREZHSaRRghg4dinbt2sHHx0fteHR0NJ49e6Z2vHr16nByckJUVBQAICoqCrVr14a9vb10jq+vL9LS0nDu3DnpnBdr+/r6SjUKk5WVhbS0NLUbERERvZ0MivoP1q1bh5MnT+LEiRMF7ktKSoKRkRFsbGzUjtvb2yMpKUk6J394ybs/775XnZOWlobMzEyYmpoWeOwZM2Zg8uTJRX06REREpEBF6oFJTEzEiBEj8Pvvv8PExERXbdLI+PHjkZqaKt0SExOLu0lERESkI0UKMNHR0bh79y7q1asHAwMDGBgYIDw8HPPnz4eBgQHs7e3x9OlTpKSkqP275ORkODg4AAAcHBwKzErK+/m/zrGysiq09wUAjI2NYWVlpXYjIiKit1ORAkzLli1x5swZxMTESLcGDRrA399f+v+GhobYv3+/9G8uXbqEhIQEeHl5AQC8vLxw5swZ3L17Vzpn3759sLKygru7u3RO/hp55+TVICIiondbkcbAWFpaolatWmrHzM3NYWdnJx0PCAhAUFAQSpUqBSsrKwQGBsLLywuNGjUCALRu3Rru7u7o2bMnfvjhByQlJeHbb7/F0KFDYWxsDAAYNGgQFi5ciK+++gr9+vXDgQMHsGHDBuzYsUOO50xEREQKV+RBvP9l7ty50NPTQ5cuXZCVlQVfX1/8/PPP0v36+vrYvn07Bg8eDC8vL5ibm6N3796YMmWKdI6rqyt27NiBUaNGYd68eahQoQKWLl0KX19fuZtLRERECqQSQojiboQupKWlwdraGqmpqbKPh3EZJ19P0PWZ7WSrRUREpHSv+/nNvZCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcYoUYH755Rd4eHjAysoKVlZW8PLywq5du6T7nzx5gqFDh8LOzg4WFhbo0qULkpOT1WokJCSgXbt2MDMzQ9myZTFmzBhkZ2ernRMWFoZ69erB2NgYbm5uCAkJ0fwZEhER0VunSAGmQoUKmDlzJqKjo/HPP//A29sbHTt2xLlz5wAAo0aNwrZt27Bx40aEh4fj9u3b6Ny5s/Tvc3Jy0K5dOzx9+hRHjhxBaGgoQkJCMGHCBOmc+Ph4tGvXDi1atEBMTAxGjhyJ/v37Y8+ePTI9ZSIiIlI6lRBCaFOgVKlS+PHHH/HJJ5+gTJkyWLNmDT755BMAwMWLF1GjRg1ERUWhUaNG2LVrFz766CPcvn0b9vb2AIDFixdj7NixuHfvHoyMjDB27Fjs2LEDZ8+elR7j888/R0pKCnbv3v3a7UpLS4O1tTVSU1NhZWWlzVMswGXcDtlqXZ/ZTrZaRERESve6n98aj4HJycnBunXrkJGRAS8vL0RHR+PZs2fw8fGRzqlevTqcnJwQFRUFAIiKikLt2rWl8AIAvr6+SEtLk3pxoqKi1GrknZNX42WysrKQlpamdiMiIqK3U5EDzJkzZ2BhYQFjY2MMGjQIf/75J9zd3ZGUlAQjIyPY2NionW9vb4+kpCQAQFJSklp4ybs/775XnZOWlobMzMyXtmvGjBmwtraWbhUrVizqUyMiIiKFKHKAqVatGmJiYnDs2DEMHjwYvXv3xvnz53XRtiIZP348UlNTpVtiYmJxN4mIiIh0xKCo/8DIyAhubm4AgPr16+PEiROYN28ePvvsMzx9+hQpKSlqvTDJyclwcHAAADg4OOD48eNq9fJmKeU/58WZS8nJybCysoKpqelL22VsbAxjY+OiPh0iIiJSIK3XgcnNzUVWVhbq168PQ0ND7N+/X7rv0qVLSEhIgJeXFwDAy8sLZ86cwd27d6Vz9u3bBysrK7i7u0vn5K+Rd05eDSIiIqIi9cCMHz8efn5+cHJywqNHj7BmzRqEhYVhz549sLa2RkBAAIKCglCqVClYWVkhMDAQXl5eaNSoEQCgdevWcHd3R8+ePfHDDz8gKSkJ3377LYYOHSr1ngwaNAgLFy7EV199hX79+uHAgQPYsGEDduyQb+YPERERKVuRAszdu3fRq1cv3LlzB9bW1vDw8MCePXvQqlUrAMDcuXOhp6eHLl26ICsrC76+vvj555+lf6+vr4/t27dj8ODB8PLygrm5OXr37o0pU6ZI57i6umLHjh0YNWoU5s2bhwoVKmDp0qXw9fWV6SkTERGR0mm9DkxJxXVgiIiIlEfn68AQERERFRcGGCIiIlIcBhgiIiJSHAYYIiIiUhwGGCIiIlIcBhgiIiJSnCJvJUC6JdcUbU7PJiKitxl7YIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcQyKuwH05riM2yFLnesz28lSh4iISFPsgSEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsUpUoCZMWMG3nvvPVhaWqJs2bLo1KkTLl26pHbOkydPMHToUNjZ2cHCwgJdunRBcnKy2jkJCQlo164dzMzMULZsWYwZMwbZ2dlq54SFhaFevXowNjaGm5sbQkJCNHuGRERE9NYpUoAJDw/H0KFDcfToUezbtw/Pnj1D69atkZGRIZ0zatQobNu2DRs3bkR4eDhu376Nzp07S/fn5OSgXbt2ePr0KY4cOYLQ0FCEhIRgwoQJ0jnx8fFo164dWrRogZiYGIwcORL9+/fHnj17ZHjKREREpHQGRTl59+7daj+HhISgbNmyiI6ORtOmTZGamoply5ZhzZo18Pb2BgCsWLECNWrUwNGjR9GoUSPs3bsX58+fx99//w17e3t4enri+++/x9ixYzFp0iQYGRlh8eLFcHV1xezZswEANWrUQGRkJObOnQtfX1+ZnjoREREplVZjYFJTUwEApUqVAgBER0fj2bNn8PHxkc6pXr06nJycEBUVBQCIiopC7dq1YW9vL53j6+uLtLQ0nDt3Tjonf428c/JqFCYrKwtpaWlqNyIiIno7aRxgcnNzMXLkSDRp0gS1atUCACQlJcHIyAg2NjZq59rb2yMpKUk6J394ybs/775XnZOWlobMzMxC2zNjxgxYW1tLt4oVK2r61IiIiKiE0zjADB06FGfPnsW6devkbI/Gxo8fj9TUVOmWmJhY3E0iIiIiHSnSGJg8w4YNw/bt2xEREYEKFSpIxx0cHPD06VOkpKSo9cIkJyfDwcFBOuf48eNq9fJmKeU/58WZS8nJybCysoKpqWmhbTI2NoaxsbEmT4eIiIgUpkg9MEIIDBs2DH/++ScOHDgAV1dXtfvr168PQ0ND7N+/Xzp26dIlJCQkwMvLCwDg5eWFM2fO4O7du9I5+/btg5WVFdzd3aVz8tfIOyevBhEREb3bitQDM3ToUKxZswZbt26FpaWlNGbF2toapqamsLa2RkBAAIKCglCqVClYWVkhMDAQXl5eaNSoEQCgdevWcHd3R8+ePfHDDz8gKSkJ3377LYYOHSr1oAwaNAgLFy7EV199hX79+uHAgQPYsGEDduzYIfPTJyIiIiUqUg/ML7/8gtTUVDRv3hzlypWTbuvXr5fOmTt3Lj766CN06dIFTZs2hYODAzZv3izdr6+vj+3bt0NfXx9eXl7o0aMHevXqhSlTpkjnuLq6YseOHdi3bx/q1KmD2bNnY+nSpZxCTURERACK2AMjhPjPc0xMTLBo0SIsWrTopec4Oztj586dr6zTvHlznDp1qijNIyIioncE90IiIiIixdFoFhLRi1zGyTM+6frMdrLUISKitxt7YIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgiIiISHEYYIiIiEhxDIq7AUSv4jJuh2y1rs9sJ1stIiIqXuyBISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixSlygImIiED79u3h6OgIlUqFLVu2qN0vhMCECRNQrlw5mJqawsfHB1euXFE75+HDh/D394eVlRVsbGwQEBCA9PR0tXNOnz6NDz/8ECYmJqhYsSJ++OGHoj87IiIieisVOcBkZGSgTp06WLRoUaH3//DDD5g/fz4WL16MY8eOwdzcHL6+vnjy5Il0jr+/P86dO4d9+/Zh+/btiIiIwMCBA6X709LS0Lp1azg7OyM6Oho//vgjJk2ahN9++02Dp0hERERvG4Oi/gM/Pz/4+fkVep8QAsHBwfj222/RsWNHAMDKlSthb2+PLVu24PPPP8eFCxewe/dunDhxAg0aNAAALFiwAG3btsVPP/0ER0dH/P7773j69CmWL18OIyMj1KxZEzExMZgzZ45a0CEiIqJ3k6xjYOLj45GUlAQfHx/pmLW1Nd5//31ERUUBAKKiomBjYyOFFwDw8fGBnp4ejh07Jp3TtGlTGBkZSef4+vri0qVL+Pfffwt97KysLKSlpandiIiI6O0ka4BJSkoCANjb26sdt7e3l+5LSkpC2bJl1e43MDBAqVKl1M4prEb+x3jRjBkzYG1tLd0qVqyo/RMiIiKiEumtmYU0fvx4pKamSrfExMTibhIRERHpiKwBxsHBAQCQnJysdjw5OVm6z8HBAXfv3lW7Pzs7Gw8fPlQ7p7Aa+R/jRcbGxrCyslK7ERER0dtJ1gDj6uoKBwcH7N+/XzqWlpaGY8eOwcvLCwDg5eWFlJQUREdHS+ccOHAAubm5eP/996VzIiIi8OzZM+mcffv2oVq1arC1tZWzyURERKRARQ4w6enpiImJQUxMDIDnA3djYmKQkJAAlUqFkSNHYurUqfjrr79w5swZ9OrVC46OjujUqRMAoEaNGmjTpg0GDBiA48eP4/Dhwxg2bBg+//xzODo6AgC6d+8OIyMjBAQE4Ny5c1i/fj3mzZuHoKAg2Z44ERERKVeRp1H/888/aNGihfRzXqjo3bs3QkJC8NVXXyEjIwMDBw5ESkoKPvjgA+zevRsmJibSv/n9998xbNgwtGzZEnp6eujSpQvmz58v3W9tbY29e/di6NChqF+/PkqXLo0JEyZwCjUREREB0CDANG/eHEKIl96vUqkwZcoUTJky5aXnlCpVCmvWrHnl43h4eODQoUNFbR4RERG9A96aWUhERET07mCAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFMSjuBhAVF5dxO2Spc31mO1nqEBHR62MPDBERESkOAwwREREpDgMMERERKQ4DDBERESkOAwwREREpDgMMERERKQ4DDBERESkO14Eh0gGuMUNEpFvsgSEiIiLFYQ8MkcKwd4eIiD0wREREpEAMMERERKQ4vIRERBJeniIipWAPDBERESkOe2CISOfk6tkB2LtDRM+xB4aIiIgUhwGGiIiIFIcBhoiIiBSHAYaIiIgUhwGGiIiIFIezkIhI0bh2DdG7iT0wREREpDjsgSEiegld9u6w54hIO+yBISIiIsVhDwwR0VuEqx7Tu4IBhoiIXgsve1FJwgBDRETFjuGIiopjYIiIiEhx2ANDRERvNfbuvJ0YYIiIiDSkq3Cky8HYb0ug4yUkIiIiUhwGGCIiIlIcBhgiIiJSHAYYIiIiUhwGGCIiIlIcBhgiIiJSnBIdYBYtWgQXFxeYmJjg/fffx/Hjx4u7SURERFQClNgAs379egQFBWHixIk4efIk6tSpA19fX9y9e7e4m0ZERETFrMQGmDlz5mDAgAHo27cv3N3dsXjxYpiZmWH58uXF3TQiIiIqZiVyJd6nT58iOjoa48ePl47p6enBx8cHUVFRhf6brKwsZGVlST+npqYCANLS0mRvX27WY9lqvdg+uWoX9ryVWJu/67ejNv878nf9qrqs/fb8d5SzrhDi1SeKEujWrVsCgDhy5Ija8TFjxoiGDRsW+m8mTpwoAPDGG2+88cYbb2/BLTEx8ZVZoUT2wGhi/PjxCAoKkn7Ozc3Fw4cPYWdnB5VK9cbbk5aWhooVKyIxMRFWVlYlvi5rv7m6rP3m6rL2m62txDYrtbYS2/y6hBB49OgRHB0dX3leiQwwpUuXhr6+PpKTk9WOJycnw8HBodB/Y2xsDGNjY7VjNjY2umria7OystLJH4Cu6rL2m6vL2m+uLmu/2dpKbLNSayuxza/D2tr6P88pkYN4jYyMUL9+fezfv186lpubi/3798PLy6sYW0ZEREQlQYnsgQGAoKAg9O7dGw0aNEDDhg0RHByMjIwM9O3bt7ibRkRERMWsxAaYzz77DPfu3cOECROQlJQET09P7N69G/b29sXdtNdibGyMiRMnFrisVVLrsvabq8vab64ua7/Z2kpss1JrK7HNclMJ8V/zlIiIiIhKlhI5BoaIiIjoVRhgiIiISHEYYIiIiEhxGGCIiIhIcRhgZLJixQo8fizf3hVE9G7LyclBTEwM/v333+JuClGJxAAjk3HjxsHBwQEBAQE4cuRIcTeHZJCWloYtW7bgwoULWtXp3bs3IiIiZGpV4VJSUrB06VKMHz8eDx8+BACcPHkSt27d0unj0nMpKSla1xg5ciSWLVsG4Hl4adasGerVq4eKFSsiLCxM6/qk7tChQ+jRowe8vLyk18mqVasQGRmpVd3ExETcvHlT+vn48eMYOXIkfvvtN63qvmlyvf/plDzbL9KzZ8/E5s2bRYcOHYShoaGoVq2amDlzprhz545sj5GVlSUSExPFjRs31G7aGDVqVKG3oKAg8fXXX4vly5eLBw8eyPQM5BESEiK2b98u/TxmzBhhbW0tvLy8xPXr1zWu27VrV7FgwQIhhBCPHz8WVapUEYaGhsLAwEBs2rRJ47odO3YUhoaGws3NTUybNk3cvHlT41qFiY2NFWXKlBFubm7CwMBAXL16VQghxDfffCN69uwp62MpRXR0tDh9+rT085YtW0THjh3F+PHjRVZWlla1Z86cKdatWyf93LVrV6GnpyccHR1FTEyMxnXLly8vTpw4IYQQ4s8//xSOjo7i0qVL4ttvvxWNGzfWqs2dO3cWM2fOLHB81qxZ4pNPPtGqtq5ej7q0adMmYWpqKvr37y+MjY2l18yCBQuEn5+fVrU/+OADsXLlSiGEEHfu3BFWVlbCy8tLlC5dWkyePFnrtuuKrt7/dIkBRgeSkpLETz/9JGrXri0MDQ1F+/btxZYtW0ROTo5G9S5fviw++OADoaenp3ZTqVRCT09Pq7Y2b95cWFlZCXNzc1GvXj1Rr149YWFhIaytrcX7778vbGxshK2trTh37pzGjyF38KpatarYv3+/EEKII0eOCDMzM/Hrr7+K9u3bi48//ljjuvb29tIH0O+//y7c3NxERkaG+Pnnn4Wnp6fGdYUQ4u7du2L27NnCw8NDGBgYiDZt2oiNGzeKp0+falVXCCFatmwpxowZI4QQwsLCQnozPnz4sHB2dtaoZmxs7GvftNGrVy8RHh6uVY3CNGjQQHrTvXr1qjAxMRHdunUTbm5uYsSIEVrVdnFxEYcPHxZCCLF3715hY2Mj9uzZIwICAkSrVq00rmtsbCztvjtgwACpndeuXROWlpZatbl06dJqgS7P6dOnRdmyZbWqravXoxBCJCQkqO1IfOzYMTFixAjx66+/alXX09NThIaGCiHUXzMnT54U9vb2WtW2sbERFy9eFEIIMW/ePCl87tmzR7i6umpVWwghdu3aJQ4dOiT9vHDhQlGnTh3RrVs38fDhQ43r6vL9T1cYYHTk6NGjYuDAgcLY2Fi4uLgIa2tr4eLiIg4ePFjkWo0bNxZNmzYVO3fuFKdOnRIxMTFqN23MnTtXdO7cWaSmpkrHUlJSxCeffCKCg4NFRkaG6Nixo2jdunWRa+sqeJmamkoB6KuvvpJ6Gc6ePStKly6tcV0TExORkJAghBCiZ8+eYuzYsUIIIW7cuCHMzc01rvui6OhoMWzYMGFiYiJKly4tRo4cKS5fvqxxPSsrKxEXFyeEUH8zvn79ujA2NtaoZt5/o7z/fdVNG7rqncr/O5k5c6b09xsZGSkqVKigVe38fyfDhw8XAwcOFEIIcenSJWFjY6NxXScnJ7Fnzx6RnZ0tKlasKPVqnD17Vqu6eW3O+1DN78KFC8LExESr2rp6PQqhu94MU1NTER8fL4RQf81cvXpV49dMHnNzc6l2+/btpZ6vGzduaP27FkKIWrVqiR07dgghngdQY2NjMX78eNGoUSPRp08fjeu+qfc/OTHAyCgpKUn8+OOPwt3dXZiYmIjPP/9c7Nu3TwghRHp6uvjqq6+Ek5NTkeuamZmJCxcuyN1cIYQQjo6OhfaunD17Vjg6Ogohnn/g2tnZFbm2roJXmTJlxMmTJ4UQz79J5b3BxcXFafVCq1Klili/fr1IT08XZcqUkb5VxsTEaPT8C3P79m0xc+ZMUa1aNWFubi569eolWrZsKQwMDMScOXM0qpn/95H/zXjv3r0af1hfv35duv3555+icuXKYvHixVKvy+LFi0WVKlXEn3/+qVH9/HTRO2VpaSmFQh8fHxEcHCyEkOdDpFy5clIPTNWqVcWGDRuEEEJcvHhRq56SiRMnCmtra1G9enXh5OQknjx5IoQQYtmyZaJRo0Zatfm9994r9AN/4sSJol69elrV1tXrUQjd9Wa4urpK7835XzOhoaGiRo0aWrW5YcOGYuzYsSIiIkKYmJhI73VRUVGifPnyWtUWQj0gTZw4UXTp0kUI8fx9Wpveozfx/ic3BhiZfPTRR8LQ0FDUrFlTzJ07t9BxI8nJyUKlUhW5doMGDdS6DOVkbm5eaK/QwYMHhYWFhRDi+bcSTd6YdRW8unfvLurVqycCAgKEmZmZuH//vhBCiK1bt4qaNWtqXHfRokXCwMBA2NjYCA8PD+mS3/z580Xz5s01rvv06VOxadMm0a5dO2FoaCjq168vfvnlF7Ver82bN2v8LTsgIEB06tRJPH36VFhYWIhr166JGzduiLp162p9uUSI5x9+ed/48tuxY4fWH34vkqt3qkWLFqJXr15i5cqVwtDQUFy5ckUIIURYWJjGl9XyDB06VDg7OwsfHx9hZ2cnHj16JIQQYu3ataJu3bpa1d64caOYM2eO2mWTkJAQsWXLFq3q/vXXX8LAwED06tVLhISEiJCQENGzZ09hYGCgdQjV1etRCN31ZkyfPl24u7uLo0ePCktLS3Ho0CGxevVqUaZMGTF//nyt2nzw4EFhY2Mj9PT0RN++faXj48eP1/qSmhBC7ZJ+kyZNpMtp8fHxwtTUVOO6unr/0yUGGJn069dPHDly5JXn5ObmajSobf/+/cLLy0scPHhQ3L9/X6SmpqrdtNG9e3fh6uoqNm/eLBITE0ViYqLYvHmzqFSpkujRo4cQ4vkbc/369YtcW1fB699//xVDhw4VHTp0ELt27ZKOT5gwQUydOlWr2idOnBCbN2+WPpSEEGL79u0iMjJS45p2dnbC1tZWDBkyRJw6darQc/7991/h4uKiUf2UlBTh4+MjbGxshL6+vqhYsaIwNDQUTZs2Fenp6Rq3O4+JiYk4f/58gePnz5+XpUs8j5y9U7GxsaJWrVrCyspKTJo0STo+bNgw0a1bN63a+fTpU/Hjjz+K4cOHSz0PQggxZ84csWTJEo3rhoaGSr0u+WVlZUnjNbSxfft20bhxY2FmZibs7OxEixYtRFhYmNZ1dfl61FVvRm5urpg6daowNzcXKpVKqFQqYWJiIr799lut2psnOzu7wHiU+Ph4kZycrHXt9u3bC19fXzFlyhRhaGgoXXbds2ePqFKlila1dfH+p0sMMDJ4+vSp8Pb21mocw6vkvcB0MYj30aNHon///sLIyEiqa2RkJAYMGCB9+J06deqlH7yvosvgpUtZWVni4sWL4tmzZ7LUW7lypcjMzJSl1qtERkaKRYsWiVmzZknd43KoW7eu6Nmzp9rsnaysLNGzZ0+texx03Tv1oszMTK0HToeHhxf6t/Hs2TOtBiTr6ekV+gF3//59rV/nSqXr3oysrCxx7tw5cezYMbUPbW09e/ZM7Nu3TyxevFikpaUJIYS4deuWLI9x48YN0a5dO+Hh4SGWLl0qHR85cqQIDAzUur7c73+6xN2oZVKmTBkcOXIEVapUkb12eHj4K+9v1qyZ1o+Rnp6Oa9euAQAqVaoECwsLrWvq6T1fZkilUqkdF0JApVIhJydH49qHDh3Cr7/+imvXrmHjxo0oX748Vq1aBVdXV3zwwQca1Xz8+DECAwMRGhoKALh8+TIqVaqEwMBAlC9fHuPGjStyzWfPnsHU1BQxMTGoVauWRu0qzvrA83Us2rdvDyEEPDw8AACnT5+GSqXCtm3b0LBhQ41rly5dGrm5uejWrRsGDBgAT0/PAuekpKSgbt26iI+P1/hx5KSvr487d+6gbNmyascfPHiAsmXLavx3raenh+TkZJQpU0bteGxsLFq0aCGt71MSnD59+rXPzfub0VROTg7S0tJga2srHbt+/TrMzc0L/K5Kghs3bqBNmzZISEhAVlaW9D4yYsQIZGVlYfHixcXdxELp4v1P1wyKuwFvix49emDZsmWYOXOm7LXlCCj/xcLCQus3mhcdPHhQ1np5/vjjD/Ts2RP+/v44efIksrKyAACpqamYPn06du7cqVHd8ePHIzY2FmFhYWjTpo103MfHB5MmTdLoBWxoaAgnJyetwlpx1geAhg0b4tq1a/j9999x8eJFAMBnn32G7t27w9zcXKvac+fORdeuXWFiYvLSc2xsbF4rvNja2hYIyy+jTRjIC+AvevDggUa/j7p160KlUkGlUqFly5YwMPjf23JOTg7i4+PV/h5fV6lSpXD58mWULl36P383Rf19eHp6QqVS4WXff/Pu0/aLire3NzZv3qwWXoDnz61Tp044cOCARnU//vjjQn8fKpUKJiYmcHNzQ/fu3VGtWrUi1x4xYgQaNGiA2NhY2NnZqT3mgAEDNGpvfs2aNUNAQAC6du0KU1NTrevl0cX7n64xwMgkOzsby5cvx99//4369esXeCObM2eOVvV10eMAABkZGZg5cyb279+Pu3fvIjc3V+3+vF4ZTegqeE2dOhWLFy9Gr169sG7dOul4kyZNMHXqVI3rbtmyBevXr0ejRo3U3txq1qyJq1evalz3m2++wddff41Vq1ahVKlSGtcprvoAYG5ujoEDB8pe9+DBg+jUqVOBAJORkYHAwEAsX778tWsFBwfL3Dp1nTt3BvD8Q65Pnz4wNjaW7svJycHp06fRuHHjItft1KkTACAmJga+vr5qvZ9GRkZwcXFBly5dilx37ty5sLS0lP7/64a71/GmesPCwsLw9OnTAsefPHmCQ4cOaVzX2toaW7ZsgY2NDerXrw/g+crVKSkpaN26NdavX49Zs2Zh//79aNKkSZFqHzp0CEeOHIGRkZHacRcXF1lWxq5bty5Gjx6NwMBAfPrppwgICECjRo20rqur9z9dYoCRydmzZ1GvXj0Az7ve8tP2jUNXPQ4A0L9/f4SHh6Nnz54oV66crG9ygG6C16VLl9C0adMCx62trbVa0v3evXsFLgsAzz9Mtfm9LFy4EHFxcXB0dISzs3OBcHvy5EmNa7+J+sDzJdbz/jtGRUXB2dkZc+fORaVKldCxY0eN64aGhmLmzJnSB22ezMxMrFy5skgBpnfv3gCef5lYs2YNfH19YW9vr3HbXmRtbQ3geQ+MpaWl2rdfIyMjNGrUSKNv2BMnTkROTg5cXFzQunVrlCtXTpb25v0+AKBPnz6y1Mzj7Owsa70X5b9Edf78eSQlJUk/5+TkYPfu3ShfvrzG9R0cHNC9e3csXLhQutSdm5uLESNGwNLSEuvWrcOgQYMwduzYIm8tkJubW2iv082bNwv8nWsiODgYP/30E/766y+EhoaiadOmcHNzQ79+/dCzZ0+N/+Z19f6nSwwwMtHV5RJAdz0OALBr1y7s2LGjyN8yXoeugpeDgwPi4uLg4uKidjwyMhKVKlXSuL0NGjTAjh07EBgYCOB/wXPp0qXw8vLSuG7eN2xd0XX9X375BRMmTMDIkSMxdepU6c3Z1tYWwcHBGgWYtLQ0iOeTCPDo0SO1HpicnBzs3Lmz0DfT12FgYIBBgwbJvofLihUrpEsmCxYskGWcWB59fX188cUXOtt3RlfjdvI7f/48EhISCvSYdOjQoci18i5RqVQqeHt7F7jf1NQUCxYs0Lity5Ytw+HDh6XwAjwfgxQYGIjGjRtj+vTpGDZsGD788MMi127dujWCg4OlvY9UKhXS09MxceJEtG3bVuM252dgYIDOnTujc+fOuHv3Ln777Td89913+Prrr9G2bVsMHz680N/bq+jq/U+nimnwMBWBLleNdHFxKXSKrBx0tVy3rtZwOHTokLCwsBCDBg0SJiYmYsSIEaJVq1bC3Nxc/PPPPxrXVboaNWpIa4Xk/+945swZjRe4+q8VfvX19bWagtusWTNZFtl7UU5OjjA0NNTJjMP69euLv//+W/a6Qjz/fRc2w+nWrVtaT4W/evWq8PDwUFu9Of9/X01cv35dxMfHC5VKJU6cOKG2sOLt27dFdna2Vm22sbERW7duLXB869at0oy3y5cvazT7LTExUbi7u4saNWoIAwMD0ahRI2FnZyeqVasmyzTq/I4dOyYGDRokbGxshJOTk5gwYYIICAgQpqam4ssvvyxSLSW+/7EHRgt518Rfx+bNmzV+HF31OADA999/jwkTJiA0NBRmZmZa1XqRri71jBs3Drm5uWjZsiUeP36Mpk2bwtjYWLourKkPPvgAMTExmDlzJmrXro29e/eiXr16iIqKQu3atTWu+6ZER0dL3+Br1qyJunXrylI3Pj6+0FrGxsbIyMjQqObBgwchhIC3tzf++OMPtbE7RkZGcHZ2hqOjo8ZtHjJkCL788kvcvHmz0DFpmg5Y19PTQ5UqVfDgwQPZZxxOnToVo0ePxvfff19om62srIpcc/78+QCef5teunSpWq9RTk4OIiIiUL16da3aPWLECLi6umL//v1wdXXF8ePH8eDBA3z55Zf46aefNKqZd4nqxTF5cunZsycCAgLw9ddf47333gMAnDhxAtOnT0evXr0APJ/9WbNmzSLXrlChAmJjY7Fu3TqcPn0a6enpCAgIgL+/vyyDbu/evYtVq1ZhxYoVuHLlCtq3b4+1a9fC19dX6jXp06cP2rRpU6TfvxLf/ziNWgt9+/Z97XNXrFih8ePMmDEDq1evxvLly9GqVSvs3LkTN27cwKhRo/Ddd99p9aFdt25dXL16FUIIuLi4wNDQUO1+bcZPVKpUCb/99ht8fHxgaWmJ2NhYVKpUCStXrsTMmTNx/vx5jWsDwNOnTxEXF4f09HS4u7vL2qWvrTc1IwZ4/ob2+eefIywsDDY2NgCeTz1u0aIF1q1bp/VUU3d3d8yYMQMdO3ZU+++4YMECrFixQqu/kRs3bsDJyUn2a+z5Lw3kkWtmzLZt2/DDDz/gl19+kXXqev425/99aNNmV1dXAM9/zxUqVIC+vr50X94A4SlTpuD999/XuN2lS5fGgQMH4OHhAWtraxw/fhzVqlXDgQMH8OWXX+LUqVMa1waAq1evIjg4WArn7u7uGDFiBCpXrqxxzZycHMycORMLFy5EcnIyAMDe3h6BgYEYO3Ys9PX1kZCQAD09PVSoUEGr9svNyMgIlStXRr9+/dCnT59CX99paWno2LGjToc2lATsgdGCNqGkKHTV4wDodvzEgAEDMGLECCxfvhwqlQq3b99GVFQURo8eje+++07r+kZGRrCysoKVlZXG4SUtLU36ZpuWlvbKc4vyDVjXM2LyCwwMxKNHj3Du3DnUqFEDwPPxCL1798bw4cOxdu1areoHBQVh6NChePLkCYQQOH78ONauXYsZM2Zg6dKlRa734hoiZ86ceem5mvaU6HKWTK9evfD48WPUqVMHRkZGBb5VaxpIdfFhk/d7aNGiRaHTkeWQk5MjDU4tXbo0bt++jWrVqsHZ2RmXLl3SqvaePXvQoUMHeHp6SuP0Dh8+jJo1a2Lbtm1o1aqVRnX19fXxzTff4JtvvpFe9y++vp2cnF673l9//QU/Pz8YGhrir7/+euW5mowJym///v3/OTbHyspKo7+n3NxcxMXFFTojtbDe9GJXjJev3jq6XH1RCN2tGqkrulquOycnR0yePFlYWVlJ19mtra3FlClTpP07Xlf+1U9fNi5DjhWPdcnKykocP368wPFjx44Ja2trWR5j9erVws3NTfrvWL58ebVVQIvixbESL7uV1N953l5CL7uVZLpYZfWDDz6Qxht169ZNtGnTRkRGRopevXppvReSp6entCtyfmPHjtV6FWg55R9j9Kb+pu/evSsOHTokDh06JO7evat1vaioKOHq6lroa7OkvhZ5CUkmSl198U2Q+1LP+PHjsWzZMkyePFn6VhYZGYlJkyZhwIABmDZt2mvXCg8PR5MmTWBgYKDTFY+vXr2KFStW4OrVq5g3bx7Kli2LXbt2wcnJSaPr7PlZWlri0KFDBVaxPXXqFJo1a/afPUtF8fjxY6Snp2s8Qwh4/lp5XUWZrvsmvwXL5fTp06hVqxb09PT+c3VbbRaazMzMxLBhw3SyyuqePXuQkZGBzp07Iy4uDh999BEuX74MOzs7rF+/vsizYfIzMTHBmTNnCow3unz5Mjw8PPDkyRON6iYnJ2P06NHS+lcvfgzqcmFIbT1+/BjDhg3DqlWrpHbq6+ujV69eWLBggcZjGT09PVG1alVMnjy50CU18pYRKEkYYGTSqVMnWFpaYtmyZbCzs5PGCYSFhWHAgAG4cuWKxrWfPHmCBQsW4ODBg4V27RV1DIIuV+gEgH79+r3WeUVZ4yM/R0dHLF68uMCH0NatWzFkyBCNFovKzs7G9OnT0a9fP9mveYeHh8PPzw9NmjRBREQELly4gEqVKmHmzJn4559/sGnTJq3qd+zYESkpKVi7dq008PXWrVvw9/eHra0t/vzzT63q562Gmje+Jk9aWppWq6HKTU9PD0lJSShbtmyhY2DyaDKeRFeXGl9s88tWt9V23M6IESNw+PBhBAcHo02bNjh9+jQqVaqErVu3YtKkSVqPU3nRw4cPizQO7GUqVqyIOXPmoGvXrmrHN2zYgNGjRyMhIUGjun5+fkhISMCwYcMK/bDWdG2jZ8+eoU2bNli8eLFOtpUBgC+++AJ///03Fi5cqPYFbvjw4WjVqhV++eUXjeqam5sjNjYWbm5ucjZXpzgGRia6XH0xICAAe/fuxSeffIKGDRtq/aaQf4VOXYzVCAkJgbOzM+rWrfvSpca18fDhw0JnTlSvXl3j8QcGBgb48ccfpRkIcho3bhymTp2KoKAgtYWsvL29sXDhQq3rL1y4EB06dICLiwsqVqwIAEhMTEStWrWwevVqrevrajXUPKtWrcLixYsRHx8vLZIXHBwMV1fXIn2Q5A/2cs9esbW1ldZRsbGxKfQ1KDQYbBsfHy8NwtTluJ03vcqqXCtCDxgwAAMHDsS1a9ekVY4PHz6MWbNmISgoSOO6kZGRhfZaasvQ0LBI+0Rp4o8//sCmTZvQvHlz6Vjbtm1hamqKTz/9VOMA8/777yMuLo4B5l2ky9UXt2/fjp07d8q22Fz+FTrz/3+5DB48GGvXrkV8fDz69u2LHj16yLrEfZ06dbBw4UJpimiehQsXok6dOhrX9fb2Rnh4eIHp6to6c+YM1qxZU+B42bJlcf/+fa3rV6xYESdPnsTff/8t7VVUo0YN+Pj4aFVX16uhAuqL5E2bNk16DdnY2Gi0SF5mZib279+Pjz76CMDzy415CygCz4PqlClTXrn3UmEOHDgg/Q3LOdg2/yUyXa5uK/cqq29qCYnvvvsOlpaWmD17NsaPHw/geQ/spEmTMHz4cI3rVqxYUSdfrgDd7osHPL+EVNhqu2XLlsXjx4+LVCv/azwwMBBffvklkpKSULt27QIzUuXeK08OvIQkk88++wzW1tb47bffYGlpidOnT6NMmTLo2LEjnJyctJqx5O7ujnXr1unsD0gXI8+zsrKwefNmLF++HEeOHEG7du0QEBCA1q1ba92DFB4ejnbt2sHJyUlaITIqKgqJiYnYuXOnRqtnAsDixYsxefJk+Pv7F7oOh6bjJipUqIANGzagcePGatOQ//zzT4wePbrE7jOSd0kDQKFv9nmrob7uJcPCuLu7Y/r06dIl2LzfzdmzZ9G8efMiB7zFixdjx44d2LZtG4Dn44Nq1qwpzRS6ePEixowZo9W3d126cuXKSy8VT5gwQeO6TZs2RdeuXREYGCi9P7m6uiIwMBBXrlzB7t27i1Qv/xISQgj8+eefsLa2RoMGDQA8X5MoJSUFnTt3lm225qNHjwBAluX49+7di9mzZ+PXX3+V/QtLYGAgVq5ciSpVquhkX7yWLVvCzs4OK1eulIJ4ZmYmevfujYcPH+Lvv/9+7VqvumwJyLf0gK4wwMjk5s2b8PX1hRACV65cQYMGDXDlyhWULl0aERERWg163LVrF+bPn4/FixfL/i3t6NGj6N69O27cuFHgj1iuP9obN24gJCQEK1euRHZ2Ns6dO6f1QN7bt29j0aJFaj0OQ4YM0WrxM7nHTeQZPXo0jh07ho0bN6Jq1ao4efIkkpOT0atXL/Tq1QsTJ07UqO7KlStf6zxNL4vl/U1UqlQJx48fV1tvwsjICGXLllVbV0QTpqamuHjxIpydndUCzJUrV+Dh4YHMzMwi1fvwww/x1VdfoX379gCgVhMAVq9ejUWLFiEqKkqrdqekpOD48eOFBg1Nf99LlizB4MGDUbp0aTg4OKgFfZVKpdV6O5GRkfDz80OPHj0QEhKCL774AufPn8eRI0cQHh4ubWioibFjx+Lhw4dYvHix9PeQk5ODIUOGwMrKCj/++KPGtfPcu3dPmpJdvXp1lC5dWqt6tra2ePz4MbKzs2FmZlagt0GbtZlatGjx0vtUKpXWY8bOnj0LX19fZGVlST3OsbGxMDY2xt69e4s0KUBXA+rfFAYYGWVnZ6utvlivXj1ZVl+8d+8ePv30U0RERMj+YnsTI88TExOxYsUKhISE4OnTp7h48WKJWnRO154+fYqhQ4ciJCQEOTk5MDAwQE5ODrp3746QkBCNQ4Cenh4sLCxgYGDwym9Q2vx9PHv2DAMHDsSECROkRdHkJPcieeXKlUNUVJT0rbpMmTI4ceKE9PPly5fx3nvvITU1VeM2b9u2Df7+/khPT4eVlVWBoKHp79vZ2RlDhgzB2LFjNW7bq1y9ehUzZ85EbGys9P40duxYrVdZLVOmDCIjI1GtWjW145cuXULjxo3x4MEDjWvn7Uq+cuVKKSjKMeMmbzbWy+ji0rqcHj9+jN9//13tC5xcK/0qCQOMAvj4+CAhIQEBAQGwt7cvEDK0ebHpauR5/ktIkZGR+Oijj9C3b1+0adPmlT0dr0sX34B1LSEhAWfPnkV6ejrq1q2r9SyFmjVrIjk5GT169EC/fv10donRxsYGp06d0kmAWbp0KSZNmoTZs2cjICAAS5cuxdWrV6VF8j7//PMi1TM1NUVMTEyBD9M8Fy9ehKenp8bTbwGgatWqaNu2LaZPny7r9htWVlaIiYnRenuQN83W1hYhISEFxitt3boVffr0wb///qtxbV3NuHlTbt68CQCyzmx88OAB7OzsADx/T1m6dCkyMzPRoUMHjS+fA89XfLe3ty9wSXj58uW4d++ezoK1Vt7YijNvuYoVK4qePXuKpUuXSpvdycXU1FTExMTIWjNPixYtxK5du2StOXjwYGFrays8PDxEcHCwuHfvnqz1//rrL2FpaSlUKpWwtrYWNjY20s3W1lar2mFhYeKjjz4SlStXFpUrVxbt27cXERERWtWU++8hv6NHj4qBAwcKa2trUb9+ffHzzz+L1NRUWR+jV69eYs6cObLWzE/ORfLc3NzEpk2bXnr/+vXrReXKlTVtqhBCCDMzM538N+3Xr5/45ZdfZKuXmpr62jdtjBo1StjZ2YnZs2dLC6v99NNPonTp0mLUqFFa1bazsxMHDx4scPzAgQOidOnSGte9cePGK2/akHOhzfxOnz4tnJ2dhZ6enqhWrZo4deqUsLe3FxYWFsLKykro6+trtYGps7OzOHz4cIHjR48eFS4uLhrX1SX2wMhk9erViIiIQFhYGOLi4lC+fHk0a9YMzZo1Q/PmzbX6tl2vXj38/PPPaNSokSxtzT/y/OrVq/j2228xZswY2Uae6+npwcnJCXXr1n3lgF1NZyfo6hvw6tWr0bdvX3Tu3Flt2fI///wTISEh6N69u0Z18/ZTyftbaNasmew9XpmZmdi4cSNWrFiB48ePo1OnTli+fDmMjY21rj116lTMnj0bLVu2LHRQojazQfKTY5G8ESNG4O+//0Z0dHSBmUaZmZlo0KABfHx8MG/ePI0fo3Pnzvj888/x6aefalwjT/6ZdBkZGZgzZw7atWtX6GuxqL/n/IOwX0bIMEAzNzcXP/30E+bNm4c7d+4AeH4pb8SIEfjyyy+1GidlZmaG6OhoaYuMPOfOnUPDhg013kz0v3432vw+5FxoMz8/Pz8YGBhg3LhxWLVqFbZv3w5fX18sWbIEwPPBw9HR0Th69KhG9U1MTHDhwoUCPa3Xrl2Du7u7Vr2WusIAowN37txBeHg4tm/fjvXr1790ivXr2rt3LyZPnoxp06YV+sZW1F1qdT3yvE+fPq8100jT2Qnm5uY4c+aM7F3tNWrUwMCBAzFq1Ci143PmzMGSJUukzeSK6tatWwgLC0N4eDjCw8Nx5coVODo6olmzZmjRogX69+8vR/MBABEREZg4cSIiIiJw//59Wfa+edWlI5VKhWvXrmlce+rUqfD395ft8lRycjI8PT1hZGSEYcOGoWrVqgCej8dYuHAhsrOzcerUqUKnob5K/pV97927hylTpqBv376Fvh6LMlvtdZ+3Jr/n/1pZOj9tVpnO72X7CmlKzhk3+cXGxqr9/OzZM5w6dQpz5szBtGnTijRN/EW6WGgTUN80M2/81YkTJ6QB2BcvXkSjRo2QkpKiUf0qVapg4sSJ6NGjh9rxVatWYeLEiVq9znWmGHt/3joZGRliz549Yvz48aJRo0bC2NhYeHp6ipEjR2pVN/9+FHLs0XP9+vXXvpVEH3/8sVi/fr3sdY2MjMSVK1cKHL9y5YowNjaW7XEuX74sevfuLQwMDGTZY+TmzZti2rRpws3NTZQrV06MGTNGXLhwQYaW6p6Hh4fQ09MTXl5eYtGiRbJcbrx27Zrw9fVV29NFT09P+Pr6anzp57/2bSrpe8Yo1ZkzZ4Sjo6Ows7MT3t7ewtvbW9jZ2QlHR0dx9uxZ2R9v+/btolmzZlrVMDY2FpcuXSpw/OLFi8LExETjuvn3WxJCCAsLC7W/56SkJK3+/mbNmiXs7OzE8uXLpff/ZcuWCTs7OzF9+nSN6+oSF7KTSePGjXHq1CnUqFEDzZs3x7hx49C0aVNZvgHLvUtt/ulwERERaNy4MQwM1P8UsrOzceTIkRIzdS7/N+B27dphzJgxOH/+vNbfgPOrWLEi9u/fX+Dyzt9//y2tcKuJx48fIzIyEmFhYQgLC8OpU6dQvXp1DBs2TG01zaLasGEDVqxYgfDwcPj6+mL27Nlo166d1lObX0X8f6+dtmv55ImNjcW5c+fw+++/46effsLIkSPRqlUr+Pv7o1OnThpdInR1dcXu3bvx8OFDxMXFAQDc3Ny0WkxR7pV9X0d2djaePHki24w9XQ181+W+QrVq1cKVK1fUZtx069ZNZzNuqlWrhhMnTmhVQ1cLbQIFX3dyvQ4BYMyYMXjw4AGGDBkirbxtYmKCsWPHarVXli7xEpJMSpUqBT09PbRu3RrNmzdH8+bNpe7rkkxfX19aIj2/Bw8eoGzZsiVm8aLXnbmkzfX8X375BSNHjkS/fv3Uli0PCQnBvHnz8MUXX2hU18jICLa2tvD390fz5s3x4YcfyhJs88Ya+fv7v/KSiBxjVFauXIkff/xR2tOratWqGDNmDHr27Kl17fwOHz6MNWvWYOPGjXjy5ImsG1FqKyoqCg8ePJBW+QWe/14mTpyIjIwMdOrUCQsWLCjyuKNt27bhwYMH6NOnj3Rs2rRp+P7775GdnQ1vb2+sX79eq78ZXU39BnS3rxCguxk3L/5dCSFw584dTJo0CRcvXkRMTIzGtXW10Kaenh78/Pykv69t27bB29tbGpOWlZWF3bt3a/2enZ6ejgsXLsDU1BRVqlSRZRydrjDAyEQIgTNnzkhjHSIiImBkZCSNcxgwYIBW9VNSUrBs2TJpHEbNmjXRr18/rddp0dPTQ3JystoiZcDz9TIaNGhQoj5A3oQ///wTs2fPln7PNWrUwJgxY7R6E+7UqRMiIyNhZGQkhVs5Aq6Li8t/fgPTdowK8HwM0HfffYdhw4apDUpctGgRpk6dWmDMkDZiYmKwevVqrFu3Dg8ePCjyQna61KZNG7Ro0UKaTnrmzBnUq1cPffr0QY0aNfDjjz/iiy++wKRJk4pUt0WLFvjkk08wdOhQAMCRI0fw4YcfYsqUKahRowa++eYb+Pn5abWCq64GvgMv3w1dG2fOnEH79u2RmJiIKlWqYN26dWjTpg0yMjKgp6eHjIwMbNq0CZ06ddKofmGDeIUQqFixItatWycFD03pYqHN/Ksfv4qmYwv79euHefPmFVjpOG8tHk0339WpYrt49RbLzc0VJ06ckG2cw4kTJ0SpUqVE+fLlxccffyw+/vhjUaFCBWFnZyeio6M1qplXR09PT7Rt21b6+eOPPxYdOnQQLi4uwtfXV6t2y+3IkSNi27ZtasdCQ0OFi4uLKFOmjBgwYIB48uRJMbXuv8XGxor58+eLLl26iLJlywpHR0fRvXv34m7Wf3JxcRGhoaEFjoeEhMgyvfLatWti6tSpwt3dXejr6wtvb2+xdOlSkZKSonVtOTk4OIgTJ05IP3/99deiSZMm0s8bNmwQNWrUKHLdMmXKiJMnT0o/jxo1Su21t2PHDuHm5qZhq5/T1dRvIYSoUaOGWvvl0KZNG/HRRx+JyMhI8cUXX4jy5cuLfv36iZycHJGTkyOGDBki3n//fY3rh4WFqd0iIiLEhQsXxLNnz2R8Fsqip6enNsYmz71794S+vn4xtOi/McDIJDo6WsyePVu0b99e2NraCgMDA1G3bl0xatQosWXLFq1qf/DBB6JPnz5qL65nz56J3r17iw8//FCjmn369BF9+vQRKpVKfPbZZ9LPffr0EQMHDhTTp0+Xff0Wbfn6+oqZM2dKP58+fVoYGBiI/v37i9mzZwsHBwcxceJEjesnJCSIxMRE6edjx46JESNGiF9//VWbZktyc3NFdHS0+Omnn0S7du2EgYFBiX1jyM/Y2LjQwc2XL1/WenDz+++/L/T09ISnp6f48ccfxc2bN7Wqp0vGxsYiISFB+rlJkyZi6tSp0s/x8fHCwsKiyHVNTEzU1h557733xA8//CD9fP36dWFmZqZhq5/T1cB3IYTYs2ePaN26tYiPj5etpp2dnYiNjRVCCPHo0SOhUqnEP//8I91/4cIFYW1tLdvjyWnXrl3i0KFD0s8LFy4UderUEd26dRMPHz4sxpYVLjU1VaSkpAiVSiXi4uLU1gd6+PChCA0NFeXKlSvuZhaKAUYm+vr6okGDBuLLL78Uf/31l6zfHk1MTAqdVXLu3DlhamqqVe1JkyaJ9PR0rWq8Kbr6Bpzngw8+ECtXrhRCCHHnzh1haWkpvLy8ROnSpcXkyZM1rvtisK1fv74YNWqU2Lp1a4l8Q3tRzZo1xbRp0woc//7770WtWrW0qv3111+Lc+fOaVXjTXFychLh4eFCCCGysrKEqamp+Pvvv6X7T58+rdFCipUrVxa7d+8WQjz/sDYyMhKRkZHS/dHR0Rot2rZ161bptnTpUuHk5CQmTpwoNm3apHbf1q1bi1w7PxsbG2FkZCT09PSEhYWFsLW1VbtpQtczboQQIi4uTgwbNky0bNlStGzZUgQGBoq4uDitagohRK1atcSOHTuEEM//JoyMjKSZqX369NG6vtwKm+Ga/6avr68W1EsSzkKSycOHD2Vb++BFVlZWSEhIQPXq1dWOJyYmar0za95Ggvk3S6tWrVqBMTElwb///qs2WDU8PBx+fn7Sz++99x4SExM1rn/27Fk0bNgQwPMZPrVr18bhw4exd+9eDBo0SOPdgNeuXYtmzZph4MCB+PDDD2XZX+pNmjx5Mj777DNERESoLfC3f/9+bNiwQava+Rf1EjLPcJJb27ZtMW7cOMyaNQtbtmyBmZmZ2oDM06dPo3LlykWu27VrV4wcORJff/01du7cCQcHB7VFK//555+Xbo3wKoWND5kyZUqBY9ouZBccHKzxv30VXc642bNnDzp06ABPT0+1v+maNWti27ZtaNWqlca14+Pj4e7uDgD4448/0L59e0yfPh0nT55E27ZtZWm/nA4ePAghBLy9vfHHH3+ozdYzMjKCs7OzVmN3dKq4E9Tb5N9//xVLliwR48aNEw8ePBBCPP/2pG23eGBgoKhQoYJYt26dSEhIEAkJCWLt2rWiQoUKYsSIEVrVzsjIEH379hX6+vrSWhYGBgaiX79+IiMjQ6vactPVN+A85ubmUjd4+/btpctVN27c0Gr9hrfBP//8I/z9/UW9evVEvXr1hL+/v2zjHkJDQ0WtWrWEsbGxMDY2FrVr15Z6wkqSe/fuiQ8//FCoVCphaWkpNm/erHa/t7e3+Prrr4tc9/Hjx6Jnz57CxsZGVK9evcDWFc2bN1e7dPouUKlUamPzDAwMROvWraWf27Ztq1UPjKenpxg7dmyB42PHjhV169bVpunC1tZW6lVs0qSJdAk6Pj5e6x5zXbp+/bpWWx0UBwYYmcTGxorSpUsLNzc3YWBgIHV3fvPNN6Jnz54a1bx27ZoQ4vmH9fDhw6VuWpVKJYyNjcXIkSO1HrQ6cOBAUalSJbFz507puueOHTtE5cqVxaBBg7SqLbdBgwYJLy8vERERIYKCgoSdnZ3IysqS7l+9erVo0KCBxvUbNmwoxo4dKyIiIoSJiYm0/1RUVJQoX768Vm2PiIgQ/v7+olGjRlKgXblypdq18nfR7NmzhZmZmfjqq6+kyxljxowRZmZmOt1/SRspKSkiOzu7wPEHDx6o/T2WJKGhoYW+V2RlZRU6QFtTmZmZsuyzlH9M3qtumjI2NhaXL18ucPzSpUtaj+tq37698PX1FVOmTBGGhobS633Pnj2iSpUqWtV+EzIyMsSFCxdEbGys2q0kYoCRScuWLcWYMWOEEOrXaw8fPiycnZ01qqlSqYSLi4vo27evWLlypUhISBCnT58Wp0+flq13RFebpemCrr4B5zl48KCwsbERenp6om/fvtLx8ePHi48//ljjups2bRKmpqaif//+wtjYWPrbWLBggfDz89O47otmz56tNkYoNzdX6x66PNnZ2WLjxo1iypQpYsqUKWLTpk2yzNjQ9QwnpZoxY4b4999/Zav3shkm9+/f13osSXp6uhg6dKgoU6ZMoWMoSqIKFSqIDRs2FDi+fv16UbFiRa1q37hxQ7Rr1054eHiobUo6cuRIERgYqFVtXbp7965o167dS8fClEQMMDKxsrKSBoDlDzDXr1/XONEfPHhQTJw4UTRr1kyYmJgIPT094ebmJgYOHCjWrVsnkpKStG63qampOH/+fIHjZ8+e1Xrmg67o8htwdnZ2gYG18fHxhb75vy5PT0/pQzr/38bJkyeFvb295o19Qe3atYWpqamoVauW2LJli+jQoYMsMzXOnj0rKlWqJMzMzETdunVF3bp1hbm5uXBxcRFnzpzRqrYuZzgpmaWlpazTnlUqlbh7926B4zExMVrv4D5kyBBRo0YNKagvX75cfP/996JChQpi9erVWtXWlcmTJwsbGxsxc+ZMERERISIiIsSMGTOEjY2NmDJlSnE3r1h0795dNGnSRJw4cUKYm5uLvXv3ilWrVolq1aqJ7du3F3fzCsUAI5P8aznk/5Dau3evqFChgtb1MzMzxf79+8V3330nPvzwQ2FsbCz09PSEu7u7VnW9vb1F165dRWZmpnTs8ePHomvXrqJly5baNltRHj9+rNazdf36dTF37lxphoimTE1NpbE1+f82rl69KvuHdGZmppg5c6bUS3Xx4kWtazZq1Ei0b99eLdg9fPhQdOjQQXh5eWlVW5cznJTsxVk3mvL09BR169YVenp6onbt2lIArVu3rvDw8BCWlpaia9euWj1GxYoVpV5cS0tLKZCuXLlS1h5GOeXm5oo5c+aI8uXLS2P/ypcvL4KDg7WuHR0dLU6fPi39vGXLFtGxY0cxfvz4EnuJUYjnszyPHTsmhHj+3zFvP6etW7eqzfYsSRhgZBIQECA6deoknj59KiwsLMS1a9fEjRs3RN26dWXrxhfi+TXrAwcOiDFjxggrKyutu/Zetlla+fLldbJZWknWqlUr8csvvwghng/Itre3FxUqVBAmJibi559/1riuq6ur2LdvnxBC/YMpNDRUq2nfS5YsKbDG0OPHj0WjRo1EvXr1hIuLi/j99981rp/HxMSk0L+FM2fOaD24edOmTUJfX18aMzBlyhTh6+srDAwMClwifFtNnjy5wCVhuQLMpEmTxKRJk4RKpRKjR4+Wfp40aZKYPn26WLNmjdqHrSbMzc2ldWzKly8vfQheu3ZNmJuba/0cdC0tLU2kpaUJIZ6P/zh8+LBW9Ro0aCA2bdokhHj+JcXExER069ZNuLm5yfpZIDdLS0vpi5aTk5M0lf/atWsldvAxA4xMUlJShI+Pj7CxsRH6+vqiYsWKwtDQUDRt2lSrdVaysrJEeHi4mDRpkmjevLkwNTUVVatWFf379xcrV65UWwBLUxkZGeK3334TQUFBIigoSCxZskQ8fvxY67pKY2dnJ31QL1myRHh4eIicnByxYcMGUb16dY3rTp8+Xbi7u4ujR48KS0tLcejQIbF69WpRpkwZMX/+fI3rVq9eXe3N9unTp8LX11d88MEHIjU1Vaxdu1arQc15PDw8xP79+wsc379/vyy9JP/884/o3r27TmY4KUFh41MSEhIKvUyqqZCQELVe1rS0NPHrr7+K9957T+svQbVr1xZhYWFCiOdjAb/88kshhBDz5s3TevD7mxYTE6P17yP/cIKZM2eK1q1bCyGEiIyMlKU3XlcaNGgg9Ta3b99e9OzZU9y8eVN89dVXolKlSsXcusIxwMjs0KFDYtGiRWLWrFnSt25NtWjRQpiZmYmaNWuKIUOGiLVr14rbt2/L1FJ6kampqRQIu3btKiZNmiSEeP5hos03kNzcXDF16lRhbm4udVebmJiIb7/9Vrb25ubmis8//1y0atVK+jYfFxcnyzenHTt2iJo1a4qNGzeKxMREkZiYKDZu3Chq164tduzYofWMk3fdi4u26VJ4eLjo1auXMDc3F1WqVBFjx44Vx48f16rmnDlzxLx584QQQuzbt0+YmJhIl7jluCTzJskRYCwtLaUZTj4+PtLvoKQux5A323XVqlVixYoVQojnXypKly4t9PT0hImJiVi3bl0xtvDluJljCWZoaIhy5cqhU6dOaN68OZo1aybtzKqNv/7667XP7dChg9aPpxQeHh7o378/Pv74Y9SqVQu7d++Gl5cXoqOj0a5dOyQlJWlV/+nTp4iLi0N6ejrc3d1hYWGhVT1XV1eMGTMG/fr1w5AhQ5CSkoJ169bByMgIALB//37069cPN27c0Opx8u8EnreYmHhh0TkhRJEWRCtsM70XqVQqZGdna9JkRXnZhqpySUpKQkhICJYtW4a0tDR8+umnWLx4MWJjY6UF1+R048YNREdHw83NDR4eHrLX16XY2FjUq1dPq4X9vL29UbFiRfj4+CAgIADnz5+Hm5sbwsPD0bt3b1y/fl2+BstAT08Pzs7OaNGihXSrUKECHj9+jIsXL8LJyQmlS5cu7mYWiivxamn+/Pn/eY6BgQEcHBzwwQcfoGzZsq9dOyUlBYcOHUJYWBhmzZqFbt26oWrVqmjWrJkUaDR503vdHVy1XaFTaSZMmIDu3btj1KhR8Pb2lnak3bt3L+rWrVvkep07d/7Pc/L+Nlq1aoX27dsXqf6wYcMwbNgwjBw5Eo6OjvD09ERmZiaMjIxw+/ZtjB49Gr6+vkVu94sOHjyodY0X/fnnny+9LyoqCvPnz0dubq7sj1tSVa1a9T8D3cOHD4tct3379oiIiEC7du0QHByMNm3aQF9fH4sXL9a0qZKoqCg8ePAAH330kXRs5cqVmDhxIjIyMtCpUycsWLAAxsbGWj+WkgQHB8Pf3x9btmzBN998Azc3NwDApk2b0Lhx42JuXUEHDhxAWFgYwsLCsHbtWjx9+hSVKlWCt7c3WrRogfLlyxd3E1+KPTBacnV1/c9zcnNz8eDBA+Tm5mL16tWv9cFWmEePHiEyMhIHDx5EWFgYYmNjUaVKFZw9e1ajelRQUlIS7ty5gzp16kg9D8ePH4eVlVWBrRz+S9++ff/znNzcXNy9exfh4eEYPXp0oUu9v8qhQ4egp6cHT09PdOzYEVFRUXByckJ8fDycnJxw5MiREvvt6UWXLl3CuHHjsG3bNvj7+2PKlClwdnYu7mbpnJ6eHoKDg/9zi4nevXsXubaBgQGGDx+OwYMHo0qVKtJxQ0NDrXtg/Pz80Lx5c4wdOxYAcObMGdSrVw99+vSBu7s7fvjhB3zxxReYNGmSxo8ht//qfY6Pj0dQUJBOvrg9efIE+vr6MDQ0lL22XJ48eYIjR45Igeb48eN49uwZqlevjnPnzhV38woqzutX75KcnBwxbdo0rQaD5uTkiKNHj4oZM2aI1q1bCzMzM42v1+7fv1/UqFGj0HELKSkpwt3dvcCS5u+KK1euiN27d0sDmXNzc3X+mNu2bdN6Aa3c3FyxY8cOMXPmTLFq1SpZB2JnZmaKY8eOiW3btsm6EaAQQty6dUv0799fGBoaio8++kjrtWWURpdjYKKiokT//v2FpaWlaNiwoViwYIG4d++eMDAw0HoTTV1vrqoLeWPQXnWTY9E2XW0r86bIPdtVVxhg3qCbN28WaXXbnJwccezYMTFr1izRpk0bYWlpKfT09ETFihVFr169xIoVK8T169c1akv79u1fuVT7vHnzRKdOnTSqrVT3798X3t7e0ptY3jTWvn37iqCgIJ0+9r///qvVar+6tGvXLlGmTBnZ3+xTUlLEV199JUxNTaUtIt5FL1slV07p6eli2bJlokmTJsLQ0FAaYJs3fVgTxsbGIiEhQfq5SZMmarsWx8fHCwsLC63arUS62FZG197EbFddYIApwfICi6Ojo/D39xdLly6VZbt3IZ7P8y9sBd48Fy5c0LpHQGl69uwpfH19RWJioto6HLt379Z6wUAlc3NzE0OGDJFl5ec8s2bNEqVKlRLu7u4F1rJ517zJWUhCCHHx4kUxZswY4eDgIExMTET79u01qqPrzVWVShfbyuiSkme7cgxMCfbrr7+iRYsWqFq1quy1TUxMcPbsWWmA2Yvi4uJQu3ZtZGZmyv7YJZWDgwP27NmDOnXqwNLSErGxsahUqRKuXbsGDw8PpKenF3cTi4WVlRVOnTqFypUry1ZTT08Ppqam8PHxgb6+/kvP27x5s2yPSepycnKwbds2LF++vEgzE/MMHjwYsbGxmDVrFrZs2YLQ0FDcvn1bmgX3+++/Izg4GCdOnJC76SWatbU1Tp48icqVK6u9j9y4cQPVqlXDkydPiruJanQ12/VN4CykEuyLL77QWe3y5cu/MsCcPn0a5cqV09njl0QZGRkwMzMrcPzhw4fv3EyK/D755BOEhYXJGmB69er1n7NuSLf09fXRqVOn156V+KLvv/8enTt3RrNmzWBhYYHQ0FApvADA8uXL0bp1a5laqxzGxsZIS0srcPzy5cs6myqvDV3Ndn0T2APzjgoMDERYWBhOnDgBExMTtfsyMzPRsGFDtGjR4rWmib8t2rZti/r16+P777+HpaUlTp8+DWdnZ3z++efIzc3Fpk2biruJxeLx48fo2rUrypQpg9q1axeYRTF8+PBiahmVBKmpqbCwsCjQk/bw4UNYWFiohZp3Qf/+/fHgwQNs2LABpUqVwunTp6Ww2LRpUwQHBxd3E19JSbNdGWBkNmfOHDRt2hQNGjQA8HyBr1GjRpW4P9rk5GTUq1cP+vr6GDZsGKpVqwYAuHjxIhYtWoScnBycPHkS9vb2xdzSN+fs2bNo2bIl6tWrhwMHDqBDhw44d+4cHj58iMOHD8vaAyGn3r17IyAgAE2bNtVJ/WXLlmHQoEEwMTGBnZ2dWs+JSqXCtWvXdPK4REqUmpqKTz75BP/88w8ePXoER0dHJCUlwcvLCzt37oS5uXlxN/GVcnNzceLECRw8eBAHDx5EZGQknjx5UiLXBGOAkZmHhwfi4uJQuXJlTJ06FcuXL0d4eDhSUlKKu2kF3LhxA4MHD8aePXvUVlb19fXFokWLXmuNm7dNamoqFi5ciNjYWKSnp6NevXoYOnRoib6c1qlTJ+zcuRPOzs7o27cvevfuLeviUw4ODhg+fDjGjRuntiovkdJt3rwZTZo0UfuiFhwcjJEjR2pd+/Dhw2rvIz4+PlrX1IXc3Fz8888/CAsLw8GDB3H48GFkZGSgfPnyaqvzlsQ1mRhgdODJkyeYN28exo8fDwsLC5w4cULq4SiJ/v33X8TFxUEIgSpVqsDW1ra4m0RFdO/ePaxatQqhoaE4f/68tIx5x44dtV44q1SpUjhx4kSJ7YEi0pS1tTUyMjLQpk0bTJ06FUuXLsWSJUuQlZVV3E17Y6ysrJCRkQEHBwcprDRv3lwRr3d+ndLS0qVLsXXrVrVjQghs2bIFdevWhZ2dHaKjo4upda/H1tYW7733Hho2bPhOh5cVK1Zg48aNBY5v3LgRoaGhxdCi11emTBkEBQUhNjYWx44dg5ubG3r27AlHR0eMGjUKV65c0bh27969sX79ehlbS1QypKamIi4uDtWqVUP9+vURGhqKffv2aVVz+PDhhY4dXLhwoSw9O3L78ccfceHCBdy6dQurV69GQECAIsILwACjtdmzZ6uN0H727Bk+/vhjGBgY4ODBg5gxYwbmzp1bjC2k1zVjxoxCl90vW7Yspk+fXgwtKro7d+5g37592LdvH/T19dG2bVucOXMG7u7uGv8d5uTk4IcffkCzZs0QGBiIoKAgtRuRUmzbtg0HDhxQO+bk5IQrV67AyckJJiYmWi8d8ccff6BJkyYFjjdu3LhETgT44osvdLJUx5vAadRaunHjBipUqADgec9Lr169kJubiz179sDMzAzvvfdeydxDggpISEgodNyPs7MzEhISiqFFr+fZs2f466+/sGLFCuzduxceHh4YOXIkunfvDisrKwDPN0/s168fRo0aVeT6Z86ckTazLIkzEYhe13fffYc5c+aoHQsICMCVK1dw6NAh7N69GzNmzNBqE9QHDx4Uuq+VlZUV7t+/r3FdKogBRkv29vbYvn07+vXrhyFDhiArKwvbt2+Xpg5ev369xM6hJ3Vly5bF6dOn4eLionY8Nja2RC/sVK5cOeTm5qJbt244fvw4PD09C5zTokUL2NjYaFRfF7tRExWHy5cvq619NWbMGJw6dQoREREoU6YMmjZtqvVlHjc3N+zevRvDhg1TO75r1y5UqlRJq9qkjgFGS8OGDcOwYcMwcuRIODo6wtPTE5mZmTAyMsLt27cxevRordI8vTndunXD8OHDYWlpKU1JDg8Px4gRI/D5558Xc+tebu7cuejatWuB9Xzys7GxQXx8fJHqvs6u6SqVCn/88UeR6hIVF1tbWxw7dgxOTk6YOHEiDh8+jLCwMCncP3jwQOq11FRQUBCGDRuGe/fuwdvbGwCwf/9+zJ49u8Qtp6F0DDBa+vLLL9GwYUPo6enB09MTHTt2hKOjI5ycnBAfHw8nJyfFjJ94133//fe4fv06WrZsCQOD5y+N3Nxc9OrVq0T/Nzx48CA6depUIMBkZGQgMDAQy5cv16huYd3gRErWo0cP9OjRA0FBQUhJSYG/v78UWDIyMvDtt9/iww8/1Oox+vXrh6ysLEybNg3ff/89AMDFxQW//PILevXqpfVzoP/hNGqZCSGwa9cunDlzBuXLl0eXLl1gampa3M2iIrhy5QpiYmJgamqK2rVrl8j1D/LT19fHnTt3ULZsWbXj9+/fh4ODA7Kzs4upZUQlixACq1evhp6eHlq2bIlWrVohNTUVNWvWRExMDHJycnD8+PECl5E1de/ePZiamsLCwkKWeqSOAYZIodLS0iCEgK2tLa5cuaI21ipvo75x48bh9u3bxdhKopIrMzMTK1askL5wBgQEaL1oZXx8PLKzs1GlShW141euXIGhoaFs4Yh4CYlI0qVLFzRs2BBjx45VO/7DDz/gxIkTha4RU5xsbGygUqmgUqkKnQapUqkwefLkYmgZkTKYmppiyJAhstbs06cP+vXrVyDAHDt2DEuXLkVYWJisj/cuYw8M0f8rU6YMDhw4gNq1a6sdP3PmDHx8fJCcnFxMLStceHg4hBDw9vbGH3/8gVKlSkn3GRkZwdnZGY6OjsXYQqJ3j5WVFU6ePKk22wkA4uLi0KBBgxK5rYxSsQeG6P+lp6cXunOuoaEh0tLSiqFFr9asWTMAkAaL599kkYiKh0qlwqNHjwocT01NLZEbIioZe2CI/l/Dhg3x0UcfYcKECWrHJ02ahG3btpWoLSFOnz792ud6eHjosCVElF/79u1hamqKtWvXQl9fH8DzMWmfffYZMjIysGvXrmJu4duDAUYmiYmJUKlU0qq8x48fx5o1a+Du7o6BAwcWc+vodWzbtg2dO3dG9+7d1dZvWLNmDTZt2oROnToVbwPz0dPTg0qlwn+9fFUqFb/1Eb1B58+fR9OmTWFjYyNNyT506BDS0tJw4MAB1KpVq5hb+PZggJHJhx9+iIEDB6Jnz55ISkpCtWrVULNmTVy5cgWBgYEFvtVTybRjxw5Mnz5dmkZdp04dTJw4EaVKlSpRbzw3btx47XNL+jRwouJy9+5dXLp0CQBQrVq1AksRaOr27dtYuHAhYmNjYWpqCg8PDwwbNkxtnBppjwFGJra2tjh69CiqVauG+fPnY/369Th8+DD27t2LQYMG4dq1a8XdRCqitLQ0rF27FsuWLUN0dDR7MojeEo8ePcKQIUOwbt066XWtr6+Pzz77DIsWLdLJIo4pKSlYvXp1gS0GSHPcjVomz549g7GxMQDg77//RocOHQAA1atXx507d4qzaVREERER6N27NxwdHTF79mx4e3vj6NGjxd2sV1q1ahWaNGkCR0dHqXcmODgYW7duLeaWEZU8/fv3x7Fjx7B9+3akpKQgJSUF27dvxz///IMvvvhC1sfav38/unfvjnLlymHixImy1n7XMcDIpGbNmli8eDEOHTqEffv2oU2bNgCedyWW5I0A6bmkpCTMnDkTVapUQdeuXWFlZYWsrCxs2bIFM2fOxHvvvVfcTXypX375BUFBQWjbti1SUlKkb5Q2Njbce4WoENu3b8fy5cvh6+sLKysrWFlZwdfXF0uWLMG2bdu0rp+YmIgpU6bA1dUVrVu3BvB8R/ikpCSta9P/MMDIZNasWfj111/RvHlzdOvWDXXq1AEA/PXXX2jYsGExt45epX379qhWrRpOnz6N4OBg3L59GwsWLCjuZr22BQsWYMmSJfjmm2+kWQ8A0KBBA5w5c6YYW0ZUMtnZ2RV6mcja2hq2trYa1Xz27Bk2btwIX19fVKtWDTExMfjxxx+hp6eHb7/9Fm3atIGhoaG2Tad8uA6MTJo3b4779+8jLS1N7QUwcOBAmJmZFWPL6L/s2rULw4cPx+DBgwusnqkE8fHxqFu3boHjxsbGyMjIKIYWEZVs3377LYKCgrBq1So4ODgAeN4LO2bMGHz33Xca1SxfvjyqV6+OHj16YN26ddLnQLdu3WRrN6ljgJGREALR0dG4evUqunfvDktLSxgZGTHAlHCRkZFYtmwZ6tevjxo1aqBnz574/PPPi7tZr83V1RUxMTEFZhvt3r0bNWrUKKZWEZVcv/zyC+Li4uDk5AQnJycAQEJCAoyNjXHv3j38+uuv0rknT558rZrZ2dnS1h75e0JJdxhgZHLjxg20adMGCQkJyMrKQqtWrWBpaYlZs2YhKysLixcvLu4m0ks0atQIjRo1QnBwMNavX4/ly5cjKCgIubm52LdvHypWrAhLS8vibuZLBQUFYejQoXjy5AmEEDh+/DjWrl2LGTNmYOnSpcXdPKISRxdrOt2+fRt//PEHli1bhhEjRsDPzw89evTgCtk6xGnUMunUqRMsLS2xbNky2NnZITY2FpUqVUJYWBgGDBiAK1euFHcTqQguXbqEZcuWYdWqVUhJSUGrVq3w119/FXezXur333/HpEmTcPXqVQCAo6MjJk+ejICAgGJuGdG75+rVq1ixYgVCQ0Nx69YtdOvWDX369IG3tzd7Z2TEACMTOzs7HDlyBNWqVYOlpaUUYK5fvw53d3c8fvy4uJtIGsjJycG2bduwfPnyEh1g8jx+/Bjp6emyLchF9LZKSUnBpk2bcPXqVYwZMwalSpXCyZMnYW9vj/Lly8vyGLm5udizZw+WLVuGbdu2wdLSEvfv35elNvESkmxyc3MLXejs5s2bJfryA72avr4+OnXqVKK2EXjR1KlT4e/vD1dXV5iZmXHMFdF/OH36NHx8fGBtbY3r169jwIABKFWqFDZv3oyEhASsXLlSlsfR09ODn58f/Pz8cO/ePaxatUqWuvQcp1HLpHXr1mprbqhUKqSnp2PixIlo27Zt8TWM3nobN26Em5sbGjdujJ9//pnf8Ij+Q1BQEPr06YMrV67AxMREOt62bVtERETo5DHLlCmDoKAgndR+V/ESkkxu3rwJX19fCCFw5coVNGjQAFeuXEHp0qURERHBLn3SqXPnzuH333/HunXrcPPmTbRq1Qr+/v7o1KkTe2SIXmBtbY2TJ0+icuXKapf8b9y4gWrVquHJkyfF3UR6DQwwMsrOzsa6detw+vRppKeno169evD394epqWlxN43eIYcPH8aaNWuwceNGPHnyBGlpacXdJKISpWzZstizZw/q1q2rFmD27duHfv36ITExsbibSK+BY2BkZGBggB49ehR3M+gdZ25uDlNTUxgZGeHRo0fF3RyiEqdDhw6YMmUKNmzYAOD5Jf+EhASMHTsWXbp0KebW0eviGBgZXb16FYGBgfDx8YGPjw9GjBghTWsl0qX4+HhMmzYNNWvWRIMGDXDq1ClMnjyZe68QFWL27NnSbL3MzEw0a9YMbm5usLS0xLRp03T2uPw8kBcvIclkz5496NChAzw9PdGkSRMAz7vyY2NjsW3bNrRq1aqYW0hvq0aNGuHEiRPw8PCAv78/unXrJts0UKK3WWRkpNolfx8fH9lqW1lZ4cMPP0S/fv3QpUsXHD58GB9//DHu3r0r22O86xhgZFK3bl34+vpi5syZasfHjRuHvXv3vvZy1ERF9c0338Df3x/u7u7F3RQixXny5AmMjY1lXzH3jz/+wNmzZxESEgJbW1tcvHgRPXr0wG+//Sbr47zLGGBkYmJigjNnzhTYDPDy5cvw8PDgqHZ6I/Jezly+nOjlcnNzMW3aNCxevBjJycm4fPkyKlWqhO+++w4uLi4arWD94MEDCCFQunRptePLli3DwIEDYW5ujosXL8LR0VGup/HO4xgYmZQpUwYxMTEFjsfExHAKNencypUrUbt2bZiamsLU1BQeHh5cNIvoJaZOnYqQkBD88MMPMDIyko7XqlVL4/3DevXqhZ07d6od27FjBwIDA7FixQr4+/tjwoQJWrWb1HEWkkwGDBiAgQMH4tq1a2jcuDGA52NgZs2axcWLSKfmzJmD7777DsOGDZPGX0VGRmLQoEG4f/8+Ro0aVcwtJCpZVq5cid9++w0tW7bEoEGDpON16tTBxYsXNap59OhRtcVMIyMj0aNHD6xevRqdO3dGtWrVSvSK3ookSBa5ublizpw5onz58kKlUgmVSiXKly8vgoODRW5ubnE3j95iLi4uIjQ0tMDxkJAQ4eLiUgwtIirZTExMxPXr14UQQlhYWIirV68KIYQ4d+6cMDc316imlZWViI2NFUIIcfLkSeHo6Ch27dol3X/x4kVhYWGhZcspP/bAyESlUmHUqFEYNWqUtPYG90CiN+HOnTtSr19+jRs3xp07d4qhRUQlm7u7Ow4dOgRnZ2e145s2bULdunU1qtmoUSMEBATA19cXP//8M7799lu0adNGun/9+vWoUaOGVu0mdQwwOsDgQm+Sm5sbNmzYgK+//lrt+Pr16wsMKiciYMKECejduzdu3bqF3NxcbN68GZcuXcLKlSuxfft2jWr+/PPPGDBgAI4dO4YpU6Zg3LhxuHfvHjw9PREREYHffvsN69evl/mZvNs4C0kmycnJGD16NPbv34+7d+/ixV9rYTtVE8nhjz/+wGeffQYfHx+1NYj279+PDRs24OOPPy7mFhKVPIcOHcKUKVMQGxsrrQMzYcIEtG7dWpb6Bw4cwPjx43H69GmUL18eX331FQYOHChLbXqOAUYmfn5+SEhIwLBhw1CuXLkC01g7duxYTC2jd0F0dDTmzp2LCxcuAABq1KiBL7/8UuPucCKiko4BRiaWlpY4dOgQPD09i7spREREbz2OgZFJxYoVC1w2IiKiksHW1va1F3h8+PChjltDcmCAkUlwcDDGjRuHX3/9FS4uLsXdHHoH6Onp/ecbskqlQnZ29htqEVHJlX+NFno78BKSFl5M9BkZGcjOzoaZmRkMDQ3VzmWiJ7lt3br1pfdFRUVh/vz5yM3N5TYWRPRWYg+MFpjoqTgVNjD80qVLGDduHLZt2wZ/f39MmTKlGFpGVDJlZ2cjJycHxsbG0rHk5GQsXrwYGRkZ6NChAz744APZHzclJQU2Njay133XsQeG6C1w+/ZtTJw4EaGhofD19cWMGTNQq1at4m4WUYnSt29fGBkZ4ddffwUAPHr0CDVr1sSTJ09Qrlw5nD9/Hlu3bkXbtm01foxZs2bBxcUFn332GQDg008/xR9//AEHBwfs3LkTderUkeW5EDdz1Fp2djaysrLUjiUnJ2Py5Mn46quvEBkZWUwto3dBamoqxo4dCzc3N5w7dw779+/Htm3bGF6ICnH48GF06dJF+nnlypXIycnBlStXEBsbi6CgIPz4449aPcbixYtRsWJFAMC+ffuwb98+7Nq1C35+fhgzZoxWtUkdLyFpacCAAQUS/XvvvScl+rlz52qd6IkK88MPP2DWrFlwcHDA2rVrudYQ0X+4deuW2urU+/fvR5cuXWBtbQ0A6N27N1asWKHVYyQlJUkBZvv27fj000/RunVruLi44P3339eqNqljgNHS4cOHsXDhQunn/Ine2toaY8eOxY8//sgAQ7IbN24cTE1N4ebmhtDQUISGhhZ63ubNm99wy4hKJhMTE2RmZko/Hz16VK3HxcTEBOnp6Vo9hq2tLRITE1GxYkXs3r0bU6dOBQAIIbgiu8wYYLT0JhI9UWF69er12utaEBHg6emJVatWYcaMGTh06BCSk5Ph7e0t3X/16lU4Ojpq9RidO3dG9+7dUaVKFTx48AB+fn4AgFOnTsHNzU2r2qSOAUZLbyLRExUmJCSkuJtApCgTJkyAn58fNmzYgDt37qBPnz4oV66cdP+ff/4p7Semqblz58LFxQWJiYn44YcfYGFhAeD5rvFDhgzRqjap4ywkLbVs2RINGzaUEn3z5s1x8+ZN6UWxb98+DB48GHFxccXcUiIiunDhAvbu3QsHBwd07doVenr/m8vy22+/oWHDhtwSRiEYYLQUHh4OPz8/lCtXDnfu3EG3bt2wbNky6f4hQ4YgIyPjpeMTiIjo7bJq1Sr8+uuvuHbtGqKiouDs7Izg4GC4urpysL2MOI1aS82aNUN0dDSGDx+OFStWYMmSJWr3e3p6YtSoUcXUOiIiepN++eUXBAUFwc/PDykpKdLAXRsbGy5+KjP2wBAREcnE3d0d06dPR6dOnWBpaYnY2FhUqlQJZ8+eRfPmzXH//v3ibuJbgz0wREREMomPj0fdunULHDc2NkZGRkYxtOjtxQBDREQkE1dXV8TExBQ4vnv3btSoUePNN+gtxmnUREREMgkKCsLQoUPx5MkTCCFw/PhxrF27FjNmzMDSpUuLu3lvFY6BISIi+n+urq7w9vbG999/r/Gidr///jsmTZqEq1evAgAcHR0xefJkBAQEyNnUdx4DDBER0f+bNGkSrl+/jvDwcMTHx2tV6/Hjx0hPT0fZsmVlah3lxwDzBsiR6ImIiOh/OIj3DejduzdycnK0XqKaiIjk8/TpU1y6dAnZ2dmy1Xzw4AGGDh0Kd3d3lC5dGqVKlVK7kXzYA0NERO+Ux48fIzAwUFoh/fLly6hUqRICAwNRvnx5jBs3TuPabdu2RVxcHAICAmBvb19gw9XevXtr1Xb6H85CIiKid8r48eMRGxuLsLAwtGnTRjru4+ODSZMmaRVgDh06hMjISNSpU0eOptIrMMDIJCgoqNDjKpUKJiYmcHNzQ8eOHdmFSERUzLZs2YL169ejUaNGaj0kNWvWlGYOaap69erIzMzUton0GhhgZHLq1CmcPHkSOTk5qFatGoDn3ZL6+vqoXr06fv75Z3z55ZeIjIyEu7t7MbeWiOjdde/evUJnBmVkZBS45FNUP//8M8aNG4cJEyagVq1aMDQ0VLvfyspKq/r0PxzEK5OOHTvCx8cHt2/fRnR0NKKjo3Hz5k20atUK3bp1w61bt9C0aVNu7EhEVMwaNGiAHTt2SD/nhZalS5fCy8tLq9o2NjZIS0uDt7c3ypYtC1tbW9ja2sLGxga2trZa1SZ1HMQrk/Lly2Pfvn0FelfOnTuH1q1b49atWzh58iRat27NzbyIiIpRZGQk/Pz80KNHD4SEhOCLL77A+fPnceTIEYSHh6N+/foa127YsCEMDAwwYsSIQgfxNmvWTNvm0//jJSSZpKam4u7duwUCzL1795CWlgbgeTJ/+vRpcTSPiIj+3wcffICYmBjMnDkTtWvXxt69e1GvXj1ERUWhdu3aWtU+e/YsTp06JQ0lIN1hgJFJx44d0a9fP8yePRvvvfceAODEiRMYPXo0OnXqBAA4fvw4qlatWoytJCIiAKhcuTKWLFkie90GDRogMTGRAeYN4CUkmaSnp2PUqFFYuXKltCiSgYEBevfujblz58Lc3FzaodTT07P4GkpE9A5KS0uTBtDm9Yq/jDYDbTdu3IhJkyZhzJgxqF27doFBvB4eHhrXJnUMMDJLT0/HtWvXAACVKlWChYVFMbeIiIj09fVx584dlC1bFnp6eoXONhJCQKVSIScnR+PH0dMrODdGpVLJUpvU8RKSzCwsLJiwiYhKmAMHDkjrcB08eFBnj6PtBpD0+tgDI5OMjAzMnDkT+/fvx927d5Gbm6t2f16vDBERFZ/s7GxMnz4d/fr1Q4UKFYq7OaQFBhiZdOvWDeHh4ejZsyfKlStXoHtyxIgRxdQyIiLKz9LSEmfOnIGLi4vstVeuXPnK+3v16iX7Y76rGGBkYmNjgx07dnDHaSKiEq5jx47o3LmzTjZWfHGxumfPnuHx48cwMjKCmZkZHj58KPtjvqs4BkYmtra23OeIiEgB/Pz8MG7cOJw5cwb169eHubm52v0dOnTQuPa///5b4NiVK1cwePBgjBkzRuO6VBB7YGSyevVqbN26FaGhoTAzMyvu5hAR0UsUNlMoj65mCv3zzz/o0aMHLl68KHvtdxV7YGQye/ZsXL16Ffb29nBxcSkw9//kyZPF1DIiIsrvxUkWb4KBgQFu3779xh/3bcYAI5O81XaJiEg5njx5AhMTE9nq/fXXX2o/CyFw584dLFy4kGMkZcZLSERE9E7JycnB9OnTsXjxYiQnJ+Py5cuoVKkSvvvuO7i4uCAgIEDj2i9enlKpVChTpgy8vb0xe/ZslCtXTtvm0/97+YVAIiKit9C0adMQEhKCH374AUZGRtLxWrVqYenSpVrVzs3NVbvl5OQgKSkJa9asYXiRGQOMFkqVKoX79+8D+N8spJfdiIioZFi5ciV+++03+Pv7Q19fXzpep04drQfZTpkyBY8fPy5wPDMzE1OmTNGqNqnjJSQthIaG4vPPP4exsTFCQ0Nfea4u1hsgIqKiMzU1xcWLF+Hs7AxLS0vExsaiUqVKOH/+PBo2bIj09HSNa+ffcym/Bw8eoGzZstwLSUYcxKuF/KGEAYWISBnc3d1x6NAhODs7qx3ftGkT6tatq1XtvE0bXxQbG8veeJkxwMgoNzcXcXFxhe6F1LRp02JqFRER5TdhwgT07t0bt27dQm5uLjZv3oxLly5h5cqV2L59u0Y1bW1toVKpoFKpULVqVbUQk5OTg/T0dAwaNEiup0DgJSTZHD16FN27d8eNGzfw4q+UW6gTEZUshw4dwpQpUxAbG4v09HTUq1cPEyZMQOvWrTWqFxoaCiEE+vXrh+DgYFhbW0v3GRkZwcXFBV5eXnI1n8AAIxtPT09UrVoVkydPLnQzx/x/zERE9HYKDw9HkyZNYGDACxy6xgAjE3Nzc8TGxsLNza24m0JERMXk5MmTMDQ0RO3atQEAW7duxYoVK+Du7o5JkyapTdsm7XAatUzef/99xMXFFXcziIjoP7xs2Qs7OzuUL18ezZo1w4oVKzSq/cUXX+Dy5csAgGvXruGzzz6DmZkZNm7ciK+++krOp/HOYx+XFk6fPi39/8DAQHz55ZdISkpC7dq1C+yF5OHh8aabR0REhZgwYQKmTZsGPz8/NGzYEABw/Phx7N69G0OHDkV8fDwGDx6M7OxsDBgwoEi1L1++DE9PTwDAxo0b0axZM6xZswaHDx/G559/juDgYJmfzbuLAUYLnp6eUKlUaoN2+/XrJ/3/vPs4iJeIqOSIjIzE1KlTC8wK+vXXX7F371788ccf8PDwwPz584scYIQQ0izUv//+Gx999BEAoGLFitLCpyQPjoHRwo0bN1773BfXGyAiouJhYWGBmJiYAmMW4+Li4OnpifT0dFy9ehUeHh7IyMgoUm1vb29UrFgRPj4+CAgIwPnz5+Hm5obw8HD07t0b169fl/GZvNs4BkYLzs7O0u3GjRsoX7682jFnZ2eUL1++SEGHiIh0q1SpUti2bVuB49u2bZMWm8vIyIClpWWRawcHB+PkyZMYNmwYvvnmGykkbdq0CY0bN9au4aSGPTAy4fLRRETKsGTJEgwePBht27aVxsCcOHECO3fuxOLFixEQEIDZs2fj+PHjWL9+vSyP+eTJE+jr6xcYH0maY4CRiZ6eHpKTk1GmTBm145cvX0aDBg2QlpZWTC0jIqIXHT58GAsXLsSlS5cAANWqVUNgYKBsvSTR0dG4cOECgOdbF9SrV0+WuvQ/DDBa6ty5M4Dnc/3btGkDY2Nj6b6cnBycPn0a1apVw+7du4uriURE9IbcvXsXn332GcLDw2FjYwMASElJQYsWLbBu3boCX3JJc5yFpKW8FXaFELC0tISpqal0n5GRERo1alTkUexERKRbutq7LjAwEOnp6Th37hxq1KgBADh//jx69+6N4cOHY+3atVq1m/6HPTAymTx5MkaPHg1zc/PibgoREb2CLveus7a2xt9//4333ntP7fjx48fRunVrpKSkaFyb1LEHRiYTJ04EANy7d0/tmiq7C4mISpZBgwahQYMG2LFjR6F712kjNze30IG6hoaGBXp6SDvsgZHJ48ePMWzYMKxcuVL6I9XX10evXr2wYMECmJmZFXMLiYgI0O3edR07dkRKSgrWrl0LR0dHAMCtW7fg7+8PW1tb/Pnnn7I/5ruK68DIZNSoUQgPD8e2bduQkpKClJQUbN26FeHh4fjyyy+Lu3lERPT/dLl33cKFC5GWlgYXFxdUrlwZlStXhqurK9LS0rBgwQKdPOa7ij0wMildujQ2bdqE5s2bqx0/ePAgPv30U9y7d694GkZERGr+/PNPfPvttxgzZoxO9q4TQuDvv//GxYsXAQA1atSAj4+PVjWpIAYYmZiZmSE6OloadZ7n3LlzaNiwYZGXoyYiIt3Q0yt48YF71ykPLyHJxMvLCxMnTsSTJ0+kY5mZmZg8eTK8vLyKsWVERJRffHx8gdu1a9ek/9XEgQMH4O7uXuiipampqahZsyYOHTqkbdMpH/bAyOTs2bPw9fVFVlYW6tSpAwCIjY2FiYkJ9uzZg5o1axZzC4mISFc6dOiAFi1aYNSoUYXeP3/+fBw8eJCDeGXEACOjx48f4/fff1e77unv76+2uB0REb15f/31F/z8/GBoaIi//vrrled26NChyPWdnZ2xe/fuAsMI8ly8eBGtW7dGQkJCkWtT4RhgiIjoraenp4ekpCSULVu20DEweTQdA2NiYoKzZ8++dGp2XFwcateujczMzCLXpsJxITst/FeKz0+TRE9ERPLIv4icLhaUK1++/CsDzOnTp1GuXDnZH/ddxh4YLbwqxefHUe1ERCXfzZs3MWXKFPz2229F/reBgYEICwvDiRMnYGJionZfZmYmGjZsiBYtWmD+/PlyNfedxwBDRESE5xMv6tWrp9EXzuTkZNSrVw/6+voYNmwYqlWrBuD52JdFixYhJycHJ0+ehL29vdzNfmfxEhIREZGW7O3tceTIEQwePBjjx4+XNolUqVTw9fXFokWLGF5kxnVgtPQ6c/8jIiKKoWVERPQmOTs7Y+fOnbh//z6OHTuGo0eP4v79+9i5cydcXV2Lu3lvHfbAaCk4OBgDBgyAlZVVgfusra3xxRdfYO7cuWjatGkxtI6IiN40W1tbvPfee8XdjLcex8BoiXP/iYiUoXPnzq+8PyUlBeHh4Zx0oRDsgdFScnJygY3A8jMwMOBGjkREJYC1tfV/3t+rV6831BrSFgOMljj3n4hIGVasWFHcTSAZcRCvltq2bYvvvvtObRPHPJmZmZg4cSI++uijYmgZERHR24tjYLTEuf9ERERvHgOMDG7cuIHBgwdjz549hc795/Q5IiIieTHAyOjff/9FXFwchBCoUqUKbG1ti7tJREREbyUGGCIiIlIcDuIlIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsVhgCEiIiLFYYAhIiIixWGAISIiIsX5P8eMVFr3VfLbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "items = list(set(joined_catlist))\n",
    "counts = [joined_catlist.count(item) for item in items]\n",
    "ind = np.array(counts).argsort()[-20:][::-1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar([items[i] for i in ind], [counts[i] for i in ind])\n",
    "def get_top_n_indices(array, top_n):\n",
    "    return array.argsort()[-top_n:][::-1]\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5567,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_n_indices(array, top_n):\n",
    "    return array.argsort()[-top_n:][::-1]\n",
    "\n",
    "\n",
    "# number of features for seniment analysis\n",
    "n_features = 6\n",
    "# Number of total products (filtered)\n",
    "n_products = 5567\n",
    "\n",
    "feature_embedding_for_products = np.random.randn(n_products, n_features)\n",
    "final_product_embeddings = feature_embedding_for_products.mean(axis=1)\n",
    "top_item_ind = get_top_n_indices(final_product_embeddings, top_n=5)\n",
    "return top_item_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = [[1,2,3,4],[4,5,6,7,1],[1,8,9,7,4]]\n",
    "list(set.intersection(*map(set, lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ind = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.57079852, 1.41198506, 1.39248131, 1.34813625, 1.3433735 ])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_product_embeddings[top_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
